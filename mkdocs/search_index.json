{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThis is the documentation for the \ncfgov-refresh\n project, a redesign of the \nwww.consumerfinance.gov\n website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh.\n\n\nDisclaimer\n\n\nThis project is a work in progress.\n Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on \nwww.consumerfinance.gov\n. In-progress updates may appear on \nbeta.consumerfinance.gov\n.\n\n\nTechnology stack\n\n\nThe standard technology stack for development of cfgov-refresh within the CFPB consists of the following base:\n\n\n\n\nMac OSX\n\n\nHomebrew\n - package manager for installing system software on OSX\n\n\nPython and PIP (Python package manager)\n\n\nWordPress API data source URL\n\n\nJinja templates\n for front-end rendering\n\n\nWagtail CMS\n for content administration\n\n\nDependencies, listed below\n\n\n\n\nDependencies\n\n\n\n\nElasticsearch\n:\n  Used for full-text search capabilities and content indexing.\n\n\nNode\n and \nnpm (Node Package Manager)\n:\n  Used for downloading and managing front-end dependencies and assets.\n\n\n\n\nFor Vagrant Virtualbox usage (\n The Vagrant box is not currently working.)\n\n\n\n\nVirtualBox\n\n\nVagrant\n\n\npython \n=  2.6\n\n\nansible \n= 1.9", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "This is the documentation for the  cfgov-refresh  project, a redesign of the  www.consumerfinance.gov  website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#disclaimer", 
            "text": "This project is a work in progress.  Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on  www.consumerfinance.gov . In-progress updates may appear on  beta.consumerfinance.gov .", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/#technology-stack", 
            "text": "The standard technology stack for development of cfgov-refresh within the CFPB consists of the following base:   Mac OSX  Homebrew  - package manager for installing system software on OSX  Python and PIP (Python package manager)  WordPress API data source URL  Jinja templates  for front-end rendering  Wagtail CMS  for content administration  Dependencies, listed below", 
            "title": "Technology stack"
        }, 
        {
            "location": "/#dependencies", 
            "text": "Elasticsearch :\n  Used for full-text search capabilities and content indexing.  Node  and  npm (Node Package Manager) :\n  Used for downloading and managing front-end dependencies and assets.   For Vagrant Virtualbox usage (  The Vagrant box is not currently working.)   VirtualBox  Vagrant  python  =  2.6  ansible  = 1.9", 
            "title": "Dependencies"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation and configuration for cfgov-refresh\n\n\nClone the repository\n\n\nUsing the console, navigate to the root directory in which your projects live and clone this project's repository:\n\n\ngit clone git@github.com:cfpb/cfgov-refresh.git\n\ncd\n cfgov-refresh\n\n\n\n\n\nYou may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on \ncfgov-refresh\n and contributing to the project.\n\n\nThere are two ways to install cfgov-refresh:\n\n\n\n\nStand-alone installation\n\n\nVagrant-box installation\n\n\n\n\n\n\nDanger\n\n\nThe instruction for Vagrant are not currently working.\n\n\n\n\nStand-alone installation\n\n\nThese instructions are somewhat specific to developing on Mac OS X,\nbut if you're familiar with other Unix-based systems,\nit should be fairly easy to adapt them to your needs.\n\n\nInstall system-level requirements\n\n\nvirtualenv \n virtualenvwrapper Python modules\n\n\nInstall \nvirtualenv\n\nand \nvirtualenvwrapper\n\nto be able to create a local environment for your server:\n\n\npip install virtualenv virtualenvwrapper\n\n\n\n\n\nAutoenv module\n\n\nThis project uses a large number of environment variables.\n\n\nTo automatically define environment variables and launch the virtualenv\nupon \ncd\ning to the project folder,\n\ninstall Autoenv\n.\nWe recommend using \nHomebrew\n:\n\n\nbrew install autoenv\n\n\n\n\n\nAfter installation, Homebrew will output instructions similar to:\n\n\nTo finish the installation, \nsource\n activate.sh in your shell:\n  \nsource\n /Users/\n[\nYOUR USERNAME\n]\n/homebrew/opt/autoenv/activate.sh\n\n\n\n\n\nRun that now for your initial setup.\nAny time you run the project you\u2019ll need to run that last line, so\nif you\u2019ll be working with the project consistently,\nwe suggest adding it to your Bash profile by running:\n\n\necho\n \nsource /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh\n \n ~/.bash_profile\n\n\n\n\n\nIf you need to find this info again later, you can run:\n\n\nbrew info autoenv\n\n\n\n\n\n\n\nNote\n\n\nIf you use Zsh you\u2019ll need to use\n\nzsh-autoenv\n,\nbut we can\u2019t provide support for issues that may arise.\n\n\n\n\nMySQL\n\n\nIf you're developing on OS X, this should be installed by default,\nand you shouldn't have to do anything else to get it working.\nYou can optionally install a different version with Homebrew.\n\n\nElasticsearch\n\n\n\n\nWarning\n\n\nThese instructions are deprecated since Elasticsearch 1.7\nis no longer supported by \nbrew\n.\n\n\n\n\nInstall Elasticsearch 1.7\n\nhowever you\u2019d like. We use \nHomebrew\n for developing on OS X):\n\n\nbrew tap homebrew/versions\nbrew search elasticsearch\nbrew install homebrew/versions/elasticsearch17\n\n\n\n\n\nJust as with Autoenv, Homebrew will output similar instructions after installation:\n\n\n# To have launchd start homebrew/versions/elasticsearch17 now and restart at login:\n\n  brew services start homebrew/versions/elasticsearch17\n\n# Or, if you don\nt want/need a background service you can just run:\n\n  elasticsearch --config\n=\n/Users/\n[\nYOUR USERNAME\n]\n/homebrew/opt/elasticsearch17/config/elasticsearch.yml\n\n\n\n\n\nAny time you resume work on the project after restarting your machine,\nyou\u2019ll need to open a new tab and run that last line.\nIf you\u2019ll be working on the project consistently,\nwe suggest using the first option, so you don't have to worry about that.\nNote that some older versions of Homebrew may suggest\nusing \nlaunchctl\n instead of \nbrew services\n.\n\n\nIf you need to find this info again later, you can run:\n\n\nbrew info elasticsearch17\n\n\n\n\n\nFront-end dependencies\n\n\nThe cfgov-refresh front end currently uses the following frameworks / tools:\n\n\n\n\nGulp\n: task management for pulling in assets,\n  linting and concatenating code, etc.\n\n\nLess\n: CSS pre-processor.\n\n\nCapital Framework\n:\n  User interface pattern-library produced by the CFPB.\n\n\n\n\n\n\nNote\n\n\nIf you\u2019re new to Capital Framework, we encourage you to\n\nstart here\n.\n\n\n\n\n\n\n\n\nInstall \nNode.js\n however you\u2019d like.\n   We recommend using \nnvm\n, though.\n\n\n\n\n\n\nInstall \nGulp\n:\n\n\n\n\n\n\nnpm install -g gulp\n\n\n\n\n\n\n\nNote\n\n\nThis project requires Node.js v5.5 or higher, and npm v3 or higher.\n\n\n\n\nSet up your environment\n\n\nIf this is the first time you're setting up the project, run the following\nscript to copy \n.env_SAMPLE\n to \n.env\n, export your environment variables,\nand activate your virtualenv for the first time.\n\n\nsource\n load-env.sh\n\n\n\n\n\nEach time you start a new session for working on this project, you'll need to\nget those environment variables and get your virtualenv running again.\nIf you setup Autoenv earlier, this will happen for you automatically when you\n\ncd\n into the project directory.\n\n\nIf you prefer not to use Autoenv, just be sure to \nsource .env\n every time\nyou start a new session of work on the project.\n\n\nRun the setup script\n\n\nAt this point, your machine should have everything it needs to automate the\nrest of the setup process.\n\n\nIf you haven't cloned this repo yet, clone it to a local folder.\nBecause related projects will need to be installed as siblings to this project,\nwe recommend putting them all in their own folder, e.g., \n~/Projects/cf.gov\n.\n\n\nOnce cloned, from the project root (\n~/Projects/cf.gov/cfgov-refresh/\n),\nrun this command to complete the setup process:\n\n\nsource\n setup.sh\n\n\n\n\n\nThis will take several minutes, going through the steps in these scripts:\n\n\n\n\nfrontend.sh\n\n\nbackend.sh\n\n\n\n\nOnce complete, you should have a fully functioning local environment,\nready for you to develop against!\n\n\nThere are some \noptional setup steps\n\nthat you may want to perform before continuing.\n\n\nWant to know more about what the setup scripts are doing?\n\nRead the detailed rundown.\n\n\nGet any errors? \nSee our troubleshooting tips.\n\n\nContinue following the \nusage instructions\n.\n\n\nVagrant-box installation\n\n\n\n\nDanger\n\n\nThese instructions are not currently working, but we'd like to get them working soon. \nPRs welcome\n.\n\n\n\n\n1. Environment variables setup\n\n\nThe project uses a number of environment variables.\nThe \nsetup.sh\n script will create a \n.env\n file for you\nfrom the \n.env_SAMPLE\n file found in the repository,\nif you don't already have one.\n\n\nInside the \n.env\n file you can customize the project environment configuration.\n\n\nIf you would like to manually copy the environment settings,\ncopy the \n.env_SAMPLE\n file and un-comment each variable after\nadding your own values.\n\ncp -a .env_SAMPLE .env \n open -t .env\n\n\n\n\n\nThen load the environment variables with:\n\n. ./.env\n\n\n\n\n\n2. Fetch extra playbooks\n\n\nThe project pulls together various open source and closed source plays. The plays are\nmanaged through ansible-galaxy, a core module for this exact purpose. To download all\nthe dependencies, use this command:\n\n\nansible-galaxy install -r ansible/requirements.yml\n\n\n\n\n\n3. Launch Vagrant virtual environment\n\n\nThe project uses Vagrant to create the simulated virtual environment allowing the developer\nto work on a production-like environment while maintaining development work on the\nlocal machine. To create this virtual environment, you need to execute the following command.\n\n\nvagrant up\n\n\n\n\n\n\n\nNote\n\n\nPlease be patient the first time you run this step.\n\n\n\n\n4. Front-end Tools\n\n\nIn order to run the application, we must generate the front-end assets.\nAfter running the following commands, visit \nhttp://localhost:8001\n to see the site running.\nYou can also place the first two export commands into your \n.bashrc\n to simplify things later.\n\n\nexport\n \nCFGOV_HOME\n=\npath/to/cfgov-refresh\n\nexport\n \nPATH\n=\n$PATH\n:\n$CFGOV_HOME\n/bin\ncfgov init\ncfgov assets\ncfgov start django\n\n\n\n\n\nOptional steps\n\n\nLoad initial data into database\n\n\nThe \ninitial-data.sh\n script can be used to initialize a new database to make\nit easy to get started working on Wagtail. This script first ensures that all\nmigrations are applied to the database, and then does the following:\n\n\n\n\nCreates an \nadmin\n superuser with a password as specified in the\n\nWAGTAIL_ADMIN_PW\n environment variable, if set.\n\n\nIf it doesn't already exist, creates a new Wagtail home page named \nCFGOV\n,\nwith a slug of \ncfgov\n.\n\n\nUpdates the default Wagtail site to use the port defined by the \nDJANGO_HTTP_PORT\n environment variable, if defined; otherwise this port is set to 80.\n\n\nIf it doesn't already exist, creates a new \nwagtail-sharing\n \nSharingSite\n with a hostname and port defined by the \nDJANGO_STAGING_HOSTNAME\n and \nDJANGO_HTTP_PORT\n environment variables.\n\n\n\n\nLoad a database dump\n\n\nIf you're installing this fresh, the initial data you receive will not be\nas extensive as you'd probably like it to be.\n\n\nYou can get a database dump by:\n\n\n\n\nGoing to [GHE]/CFGOV/platform/wiki/Database-downloads\n\n\nSelecting one of the extractions and downloading the \nproduction_django.sql.gz\n file\n\n\nUnzip it\n\n\nRun:\n\n\n\n\n./refresh-data.sh /path/to/dump.sql\n\n\n\n\n\nThe \nrefresh-data.sh\n script will apply the same changes as the \ninitial-data.sh\n script described above (including setting up the \nadmin\n superuser), but will not apply migrations.\n\n\nTo apply any unapplied migrations to a database created from a dump, run:\n\n\npython cfgov/manage.py migrate\n\n\n\n\n\nInstall dependencies for working with the GovDelivery API\n\n\nInstall the following GovDelivery dependencies into your virtual environment:\n\n\npip install git+git://github.com/dpford/flask-govdelivery\npip install git+git://github.com/rosskarchner/govdelivery\n\n\n\n\n\nUncomment and set the GovDelivery environment variables in your \n.env\n file.\n\n\n\n\nNote\n\n\nGovDelivery is a third-party web service that powers our emails.\nThe API is used by subscribe forms on our website.\nUsers may decide to swap this tool out for another third-party service.\n\n\n\n\nCurious about what the setup scripts are doing?\n\n\nHere's a rundown of each of the scripts called by \nsetup.sh\n and what they do.\n\n\n1. \nfrontend.sh\n\n\n\n\nInitialize project dependency directories\n (\ninit\n)\n\n\n\n\nThis script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.\n\n\nIt will then set some env vars for the Node and Bower dependency directories.\n1. \nClean project dependencies\n (\nclean\n)\n\n\nThe script will now empty out all installed dependencies,\n   so the new installation can start fresh.\n1. \nInstall project dependencies\n (\ninstall\n)\n\n\nNode and Bower dependencies are installed.\n   The \ndevDependencies\n from \npackage.json\n are not installed\n   if the environment is production, and if it's the dev or test environment,\n   it checks to see if Protractor is globally installed.\n1. \nRun tasks to build the project for distribution\n (\nbuild\n)\n\n\nFinally, the script executes \ngulp clean\n to wipe out any lingering\n   \ndist\n files, then runs \ngulp build\n to rebuild everything.\n\n\n2. \nbackend.sh\n\n\n\n\nConfirm environment\n (\ninit\n)\n\n\n\n\nThis script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.\n\n\nIt will then run a script to ensure that you're in a virtualenv.\n   If not, the script will end, to prevent you from accidentally installing\n   your Python dependencies globally.\n1. \nInstall project dependencies\n (\ninstall\n)\n\n\nPython dependencies are installed into the virtualenv via pip.\n   Dependencies vary slightly depending on whether we're in dev, test, or prod.\n1. \nSetup MySQL server\n (\ndb_setup\n)\n\n\nFinally, the script will start the MySQL server, if it's not already running,\n   run \ncreate-mysql-db.sh\n to create the database using\n   the variables given in \n.env\n, if it's not already there,\n   and will run \ninitial-data.sh\n to create the first Wagtail user\n   and load some basic initial data.\n\n\nTroubleshooting\n\n\nHere are some common issues and how you can fix them:\n\n\nErrors referencing South, or other Python errors:\n\n\nSince moving to Django 1.8, we use Django's built-in migration engine,\nand we no longer use South.\nIf you're getting South errors, you probably have it installed globally.\nTo solve this, from outside the virtual environment, run \npip uninstall south\n.\n\n\nIf you're getting other kinds of Python errors (for example, when running tox),\nyou may even want to go as far as uninstalling all globally-installed\nPython packages: \npip freeze | grep -v \"^-e\" | xargs pip uninstall -y\n.\nAfter doing that, you'll need to reinstall virtualenv:\n\npip install virtualenv virtualenvwrapper\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation-and-configuration-for-cfgov-refresh", 
            "text": "", 
            "title": "Installation and configuration for cfgov-refresh"
        }, 
        {
            "location": "/installation/#clone-the-repository", 
            "text": "Using the console, navigate to the root directory in which your projects live and clone this project's repository:  git clone git@github.com:cfpb/cfgov-refresh.git cd  cfgov-refresh  You may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on  cfgov-refresh  and contributing to the project.  There are two ways to install cfgov-refresh:   Stand-alone installation  Vagrant-box installation    Danger  The instruction for Vagrant are not currently working.", 
            "title": "Clone the repository"
        }, 
        {
            "location": "/installation/#stand-alone-installation", 
            "text": "These instructions are somewhat specific to developing on Mac OS X,\nbut if you're familiar with other Unix-based systems,\nit should be fairly easy to adapt them to your needs.", 
            "title": "Stand-alone installation"
        }, 
        {
            "location": "/installation/#install-system-level-requirements", 
            "text": "", 
            "title": "Install system-level requirements"
        }, 
        {
            "location": "/installation/#virtualenv-virtualenvwrapper-python-modules", 
            "text": "Install  virtualenv \nand  virtualenvwrapper \nto be able to create a local environment for your server:  pip install virtualenv virtualenvwrapper", 
            "title": "virtualenv &amp; virtualenvwrapper Python modules"
        }, 
        {
            "location": "/installation/#autoenv-module", 
            "text": "This project uses a large number of environment variables.  To automatically define environment variables and launch the virtualenv\nupon  cd ing to the project folder, install Autoenv .\nWe recommend using  Homebrew :  brew install autoenv  After installation, Homebrew will output instructions similar to:  To finish the installation,  source  activate.sh in your shell:\n   source  /Users/ [ YOUR USERNAME ] /homebrew/opt/autoenv/activate.sh  Run that now for your initial setup.\nAny time you run the project you\u2019ll need to run that last line, so\nif you\u2019ll be working with the project consistently,\nwe suggest adding it to your Bash profile by running:  echo   source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh    ~/.bash_profile  If you need to find this info again later, you can run:  brew info autoenv   Note  If you use Zsh you\u2019ll need to use zsh-autoenv ,\nbut we can\u2019t provide support for issues that may arise.", 
            "title": "Autoenv module"
        }, 
        {
            "location": "/installation/#mysql", 
            "text": "If you're developing on OS X, this should be installed by default,\nand you shouldn't have to do anything else to get it working.\nYou can optionally install a different version with Homebrew.", 
            "title": "MySQL"
        }, 
        {
            "location": "/installation/#elasticsearch", 
            "text": "Warning  These instructions are deprecated since Elasticsearch 1.7\nis no longer supported by  brew .   Install Elasticsearch 1.7 \nhowever you\u2019d like. We use  Homebrew  for developing on OS X):  brew tap homebrew/versions\nbrew search elasticsearch\nbrew install homebrew/versions/elasticsearch17  Just as with Autoenv, Homebrew will output similar instructions after installation:  # To have launchd start homebrew/versions/elasticsearch17 now and restart at login: \n  brew services start homebrew/versions/elasticsearch17 # Or, if you don t want/need a background service you can just run: \n  elasticsearch --config = /Users/ [ YOUR USERNAME ] /homebrew/opt/elasticsearch17/config/elasticsearch.yml  Any time you resume work on the project after restarting your machine,\nyou\u2019ll need to open a new tab and run that last line.\nIf you\u2019ll be working on the project consistently,\nwe suggest using the first option, so you don't have to worry about that.\nNote that some older versions of Homebrew may suggest\nusing  launchctl  instead of  brew services .  If you need to find this info again later, you can run:  brew info elasticsearch17", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/installation/#front-end-dependencies", 
            "text": "The cfgov-refresh front end currently uses the following frameworks / tools:   Gulp : task management for pulling in assets,\n  linting and concatenating code, etc.  Less : CSS pre-processor.  Capital Framework :\n  User interface pattern-library produced by the CFPB.    Note  If you\u2019re new to Capital Framework, we encourage you to start here .     Install  Node.js  however you\u2019d like.\n   We recommend using  nvm , though.    Install  Gulp :    npm install -g gulp   Note  This project requires Node.js v5.5 or higher, and npm v3 or higher.", 
            "title": "Front-end dependencies"
        }, 
        {
            "location": "/installation/#set-up-your-environment", 
            "text": "If this is the first time you're setting up the project, run the following\nscript to copy  .env_SAMPLE  to  .env , export your environment variables,\nand activate your virtualenv for the first time.  source  load-env.sh  Each time you start a new session for working on this project, you'll need to\nget those environment variables and get your virtualenv running again.\nIf you setup Autoenv earlier, this will happen for you automatically when you cd  into the project directory.  If you prefer not to use Autoenv, just be sure to  source .env  every time\nyou start a new session of work on the project.", 
            "title": "Set up your environment"
        }, 
        {
            "location": "/installation/#run-the-setup-script", 
            "text": "At this point, your machine should have everything it needs to automate the\nrest of the setup process.  If you haven't cloned this repo yet, clone it to a local folder.\nBecause related projects will need to be installed as siblings to this project,\nwe recommend putting them all in their own folder, e.g.,  ~/Projects/cf.gov .  Once cloned, from the project root ( ~/Projects/cf.gov/cfgov-refresh/ ),\nrun this command to complete the setup process:  source  setup.sh  This will take several minutes, going through the steps in these scripts:   frontend.sh  backend.sh   Once complete, you should have a fully functioning local environment,\nready for you to develop against!  There are some  optional setup steps \nthat you may want to perform before continuing.  Want to know more about what the setup scripts are doing? Read the detailed rundown.  Get any errors?  See our troubleshooting tips.  Continue following the  usage instructions .", 
            "title": "Run the setup script"
        }, 
        {
            "location": "/installation/#vagrant-box-installation", 
            "text": "Danger  These instructions are not currently working, but we'd like to get them working soon.  PRs welcome .", 
            "title": "Vagrant-box installation"
        }, 
        {
            "location": "/installation/#1-environment-variables-setup", 
            "text": "The project uses a number of environment variables.\nThe  setup.sh  script will create a  .env  file for you\nfrom the  .env_SAMPLE  file found in the repository,\nif you don't already have one.  Inside the  .env  file you can customize the project environment configuration.  If you would like to manually copy the environment settings,\ncopy the  .env_SAMPLE  file and un-comment each variable after\nadding your own values. cp -a .env_SAMPLE .env   open -t .env   Then load the environment variables with: . ./.env", 
            "title": "1. Environment variables setup"
        }, 
        {
            "location": "/installation/#2-fetch-extra-playbooks", 
            "text": "The project pulls together various open source and closed source plays. The plays are\nmanaged through ansible-galaxy, a core module for this exact purpose. To download all\nthe dependencies, use this command:  ansible-galaxy install -r ansible/requirements.yml", 
            "title": "2. Fetch extra playbooks"
        }, 
        {
            "location": "/installation/#3-launch-vagrant-virtual-environment", 
            "text": "The project uses Vagrant to create the simulated virtual environment allowing the developer\nto work on a production-like environment while maintaining development work on the\nlocal machine. To create this virtual environment, you need to execute the following command.  vagrant up   Note  Please be patient the first time you run this step.", 
            "title": "3. Launch Vagrant virtual environment"
        }, 
        {
            "location": "/installation/#4-front-end-tools", 
            "text": "In order to run the application, we must generate the front-end assets.\nAfter running the following commands, visit  http://localhost:8001  to see the site running.\nYou can also place the first two export commands into your  .bashrc  to simplify things later.  export   CFGOV_HOME = path/to/cfgov-refresh export   PATH = $PATH : $CFGOV_HOME /bin\ncfgov init\ncfgov assets\ncfgov start django", 
            "title": "4. Front-end Tools"
        }, 
        {
            "location": "/installation/#optional-steps", 
            "text": "", 
            "title": "Optional steps"
        }, 
        {
            "location": "/installation/#load-initial-data-into-database", 
            "text": "The  initial-data.sh  script can be used to initialize a new database to make\nit easy to get started working on Wagtail. This script first ensures that all\nmigrations are applied to the database, and then does the following:   Creates an  admin  superuser with a password as specified in the WAGTAIL_ADMIN_PW  environment variable, if set.  If it doesn't already exist, creates a new Wagtail home page named  CFGOV ,\nwith a slug of  cfgov .  Updates the default Wagtail site to use the port defined by the  DJANGO_HTTP_PORT  environment variable, if defined; otherwise this port is set to 80.  If it doesn't already exist, creates a new  wagtail-sharing   SharingSite  with a hostname and port defined by the  DJANGO_STAGING_HOSTNAME  and  DJANGO_HTTP_PORT  environment variables.", 
            "title": "Load initial data into database"
        }, 
        {
            "location": "/installation/#load-a-database-dump", 
            "text": "If you're installing this fresh, the initial data you receive will not be\nas extensive as you'd probably like it to be.  You can get a database dump by:   Going to [GHE]/CFGOV/platform/wiki/Database-downloads  Selecting one of the extractions and downloading the  production_django.sql.gz  file  Unzip it  Run:   ./refresh-data.sh /path/to/dump.sql  The  refresh-data.sh  script will apply the same changes as the  initial-data.sh  script described above (including setting up the  admin  superuser), but will not apply migrations.  To apply any unapplied migrations to a database created from a dump, run:  python cfgov/manage.py migrate", 
            "title": "Load a database dump"
        }, 
        {
            "location": "/installation/#install-dependencies-for-working-with-the-govdelivery-api", 
            "text": "Install the following GovDelivery dependencies into your virtual environment:  pip install git+git://github.com/dpford/flask-govdelivery\npip install git+git://github.com/rosskarchner/govdelivery  Uncomment and set the GovDelivery environment variables in your  .env  file.   Note  GovDelivery is a third-party web service that powers our emails.\nThe API is used by subscribe forms on our website.\nUsers may decide to swap this tool out for another third-party service.", 
            "title": "Install dependencies for working with the GovDelivery API"
        }, 
        {
            "location": "/installation/#curious-about-what-the-setup-scripts-are-doing", 
            "text": "Here's a rundown of each of the scripts called by  setup.sh  and what they do.", 
            "title": "Curious about what the setup scripts are doing?"
        }, 
        {
            "location": "/installation/#1-frontendsh", 
            "text": "Initialize project dependency directories  ( init )   This script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.  It will then set some env vars for the Node and Bower dependency directories.\n1.  Clean project dependencies  ( clean )  The script will now empty out all installed dependencies,\n   so the new installation can start fresh.\n1.  Install project dependencies  ( install )  Node and Bower dependencies are installed.\n   The  devDependencies  from  package.json  are not installed\n   if the environment is production, and if it's the dev or test environment,\n   it checks to see if Protractor is globally installed.\n1.  Run tasks to build the project for distribution  ( build )  Finally, the script executes  gulp clean  to wipe out any lingering\n    dist  files, then runs  gulp build  to rebuild everything.", 
            "title": "1. frontend.sh"
        }, 
        {
            "location": "/installation/#2-backendsh", 
            "text": "Confirm environment  ( init )   This script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.  It will then run a script to ensure that you're in a virtualenv.\n   If not, the script will end, to prevent you from accidentally installing\n   your Python dependencies globally.\n1.  Install project dependencies  ( install )  Python dependencies are installed into the virtualenv via pip.\n   Dependencies vary slightly depending on whether we're in dev, test, or prod.\n1.  Setup MySQL server  ( db_setup )  Finally, the script will start the MySQL server, if it's not already running,\n   run  create-mysql-db.sh  to create the database using\n   the variables given in  .env , if it's not already there,\n   and will run  initial-data.sh  to create the first Wagtail user\n   and load some basic initial data.", 
            "title": "2. backend.sh"
        }, 
        {
            "location": "/installation/#troubleshooting", 
            "text": "Here are some common issues and how you can fix them:", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/installation/#errors-referencing-south-or-other-python-errors", 
            "text": "Since moving to Django 1.8, we use Django's built-in migration engine,\nand we no longer use South.\nIf you're getting South errors, you probably have it installed globally.\nTo solve this, from outside the virtual environment, run  pip uninstall south .  If you're getting other kinds of Python errors (for example, when running tox),\nyou may even want to go as far as uninstalling all globally-installed\nPython packages:  pip freeze | grep -v \"^-e\" | xargs pip uninstall -y .\nAfter doing that, you'll need to reinstall virtualenv: pip install virtualenv virtualenvwrapper .", 
            "title": "Errors referencing South, or other Python errors:"
        }, 
        {
            "location": "/usage/", 
            "text": "Usage\n\n\nIf not using the Vagrant box, you will generally have four tabs\n(or windows) open in your terminal, which will be used for:\n\n\n\n\nGit operations\n.\n    Perform Git operations and general development in the repository,\n    such as \ngit checkout master\n.\n\n\nElasticsearch\n.\n    Run an Elasticsearch (ES) instance.\n    See instructions \nbelow\n.\n\n\nDjango server\n. Start and stop the web server.\n    Server is started with \n./runserver.sh\n,\n    but see more details \nbelow\n.\n\n\nGulp watch\n.\n    Run the Gulp watch (\ngulp watch\n) task to automatically re-run the gulp\n    asset compilation tasks when their source files are changed.\n\n\n\n\nWhat follows are the specific steps for each of these tabs.\n\n\n1. Git operations\n\n\nFrom this tab you can do Git operations,\nsuch as checking out our master branch:\n\n\ngit checkout master\n\n\n\n\n\nUpdating all dependencies\n\n\nEach time you fetch from the upstream repository (this repo), run \n./setup.sh\n.\nThis setup script will remove and reinstall the project dependencies\nand rebuild the site's JavaScript and CSS assets.\n\n\n\n\nNote\n\n\nYou may also run \n./backend.sh\n or \n./frontend.sh\n\nif you only want to re-build the backend or front-end, respectively.\n\n\n\n\n2. Run Elasticsearch\n\n\n\n\nNote\n\n\nThis Elasticsearch tab (or window) might not be necessary if you opted for the \nlaunchd\n option when \ninstalling Elasticsearch\n.\n\n\n\n\nTo launch Elasticsearch, first find out where your Elasticsearch config file is located.\nYou can do this with \nHomebrew\n using:\n\n\nbrew info elasticsearch\n\n\n\n\n\nThe last line of that output should be the command you need to launch Elasticsearch with the\nproper path to its configuration file. For example, it may look like:\n\n\nelasticsearch --config\n=\n/Users/\n[\nYOUR MAC OSX USERNAME\n]\n/homebrew/opt/elasticsearch/config/elasticsearch.yml\n\n\n\n\n\n3. Load Indexes \n Launch Site\n\n\nFirst, move into the \ncfgov-refresh\n project directory\nand ready your environment:\n\n\n# Use the cfgov-refresh virtualenv.\n\nworkon cfgov-refresh\n\n\n# cd into this directory (if you aren\nt already there)\n\n\ncd\n cfgov-refresh\n\n\n\n\n\nIndex the latest content from the API output from a WordPress and Django back-end.\n\nThis requires the constants in \nStand alone installation\n to be set.\n\n\npython cfgov/manage.py sheer_index -r\n\n\n\n\n\n\n\nNote\n\n\nTo view the indexed content you can use a tool called\n\nelasticsearch-head\n.\n\n\n\n\nFrom the project root, start the Django server:\n\n\n./runserver.sh\n\n\n\n\n\n\n\nNote\n\n\nIf prompted to migrate database changes,\nstop the server with \nctrl\n + \nc\n and run these commands:\n\n\n\n\npython cfgov/manage.py migrate\n./initial-data.sh\n./runserver.sh\n\n\n\n\n\nTo set up a superuser in order to access the Wagtail admin:\n\n\npython cfgov/manage.py createsuperuser\n\n\n\n\n\nTo view the site browse to: \nhttp://localhost:8000\n\n\n\n\nUsing a different port\n\n\nIf you want to run the server at a port other than 8000 use\n\n\npython cfgov/manage.py runserver \nport number\n\n\nSpecify an alternate port number, e.g. \n8001\n.\n\n\n\n\nTo view the Wagtail admin login,\nbrowse to: \nhttp://localhost:8000/admin/login/\n\n\n\n\nUsing HTTPS locally\n\n\nTo access a local server using HTTPS use\n\n\n./runserver.sh ssl\n\n\nYou'll need to ignore any browser certificate errors.\n\n\n\n\n4. Launch the Gulp watch task\n\n\nTo watch for changes in the source code and automatically update the running site,\nopen a terminal and run:\n\n\ngulp build\ngulp watch\n\n\n\n\n\n\n\nNote\n\n\nThe watch task only runs for the tasks for files that have changed.\nAlso, you must run \ngulp build\n at least once before watching.\n\n\n\n\nAvailable Gulp Tasks\n\n\nIn addition to \ngulp watch\n, there are a number of other important gulp tasks,\nparticularly \ngulp build\n and \ngulp test\n,\nwhich will build the project and test it, respectively.\nUsing the \ngulp --tasks\n command you can view all available tasks.\nThe important ones are listed below:\n\n\ngulp build           # Concatenate, optimize, and copy source files to the production /dist/ directory.\ngulp clean           # Remove the contents of the production /dist/ directory.\ngulp lint            # Lint the scripts and build files.\ngulp docs            # Generate JSDocs from the scripts.\ngulp test            # Run linting, unit and acceptance tests (see below).\ngulp test:unit       # Run only unit tests on source code.\ngulp test:acceptance # Run only acceptance (in-browser) tests on production code.\ngulp watch           # Watch for source code changes and auto-update a browser instance.", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/#usage", 
            "text": "If not using the Vagrant box, you will generally have four tabs\n(or windows) open in your terminal, which will be used for:   Git operations .\n    Perform Git operations and general development in the repository,\n    such as  git checkout master .  Elasticsearch .\n    Run an Elasticsearch (ES) instance.\n    See instructions  below .  Django server . Start and stop the web server.\n    Server is started with  ./runserver.sh ,\n    but see more details  below .  Gulp watch .\n    Run the Gulp watch ( gulp watch ) task to automatically re-run the gulp\n    asset compilation tasks when their source files are changed.   What follows are the specific steps for each of these tabs.", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/#1-git-operations", 
            "text": "From this tab you can do Git operations,\nsuch as checking out our master branch:  git checkout master", 
            "title": "1. Git operations"
        }, 
        {
            "location": "/usage/#updating-all-dependencies", 
            "text": "Each time you fetch from the upstream repository (this repo), run  ./setup.sh .\nThis setup script will remove and reinstall the project dependencies\nand rebuild the site's JavaScript and CSS assets.   Note  You may also run  ./backend.sh  or  ./frontend.sh \nif you only want to re-build the backend or front-end, respectively.", 
            "title": "Updating all dependencies"
        }, 
        {
            "location": "/usage/#2-run-elasticsearch", 
            "text": "Note  This Elasticsearch tab (or window) might not be necessary if you opted for the  launchd  option when  installing Elasticsearch .   To launch Elasticsearch, first find out where your Elasticsearch config file is located.\nYou can do this with  Homebrew  using:  brew info elasticsearch  The last line of that output should be the command you need to launch Elasticsearch with the\nproper path to its configuration file. For example, it may look like:  elasticsearch --config = /Users/ [ YOUR MAC OSX USERNAME ] /homebrew/opt/elasticsearch/config/elasticsearch.yml", 
            "title": "2. Run Elasticsearch"
        }, 
        {
            "location": "/usage/#3-load-indexes-launch-site", 
            "text": "First, move into the  cfgov-refresh  project directory\nand ready your environment:  # Use the cfgov-refresh virtualenv. \nworkon cfgov-refresh # cd into this directory (if you aren t already there)  cd  cfgov-refresh  Index the latest content from the API output from a WordPress and Django back-end. This requires the constants in  Stand alone installation  to be set.  python cfgov/manage.py sheer_index -r   Note  To view the indexed content you can use a tool called elasticsearch-head .   From the project root, start the Django server:  ./runserver.sh   Note  If prompted to migrate database changes,\nstop the server with  ctrl  +  c  and run these commands:   python cfgov/manage.py migrate\n./initial-data.sh\n./runserver.sh  To set up a superuser in order to access the Wagtail admin:  python cfgov/manage.py createsuperuser  To view the site browse to:  http://localhost:8000   Using a different port  If you want to run the server at a port other than 8000 use  python cfgov/manage.py runserver  port number  Specify an alternate port number, e.g.  8001 .   To view the Wagtail admin login,\nbrowse to:  http://localhost:8000/admin/login/   Using HTTPS locally  To access a local server using HTTPS use  ./runserver.sh ssl  You'll need to ignore any browser certificate errors.", 
            "title": "3. Load Indexes &amp; Launch Site"
        }, 
        {
            "location": "/usage/#4-launch-the-gulp-watch-task", 
            "text": "To watch for changes in the source code and automatically update the running site,\nopen a terminal and run:  gulp build\ngulp watch   Note  The watch task only runs for the tasks for files that have changed.\nAlso, you must run  gulp build  at least once before watching.", 
            "title": "4. Launch the Gulp watch task"
        }, 
        {
            "location": "/usage/#available-gulp-tasks", 
            "text": "In addition to  gulp watch , there are a number of other important gulp tasks,\nparticularly  gulp build  and  gulp test ,\nwhich will build the project and test it, respectively.\nUsing the  gulp --tasks  command you can view all available tasks.\nThe important ones are listed below:  gulp build           # Concatenate, optimize, and copy source files to the production /dist/ directory.\ngulp clean           # Remove the contents of the production /dist/ directory.\ngulp lint            # Lint the scripts and build files.\ngulp docs            # Generate JSDocs from the scripts.\ngulp test            # Run linting, unit and acceptance tests (see below).\ngulp test:unit       # Run only unit tests on source code.\ngulp test:acceptance # Run only acceptance (in-browser) tests on production code.\ngulp watch           # Watch for source code changes and auto-update a browser instance.", 
            "title": "Available Gulp Tasks"
        }, 
        {
            "location": "/whats-new/", 
            "text": "What\u2019s new?\n\n\nGuiding principles\n\n\n\n\nReverse the sprawl of technologies and repositories left by the last 5 years of consumerfinance.gov development. \n\n\nReduce risk by integrating earlier and implementing a single release cadence for the entire site.\n\n\nWe don\u2019t deploy code to coincide with announcements, and events. \n\n\nPrefer using and/or extending the CMS and it\u2019s primitives (page types, atomic design elements, etc) over the creation of \u201capps\u201d that own a particular URL space.\n\n\n\n\nBenefits\n\n\n\n\nBy centering most work on the cfgov-refresh, it becomes easier to understand what\u2019s happening, easier to communicate about the state of the site, easier to develop on, and simpler to deploy.\n\n\nBy working with the CMS, we empower editors to determine when a particular page or section goes live, and can provide a way to edit the incidental text on a page. \n\n\n\n\nPractical impacts\n\n\n\n\nAny new work that will appear on consumerfinance.gov should be built with Django, and (subject to guidelines below) live in the primary code repo for the site (cfgov-refresh). \n\n\nIf a particular page, section, or feature can not go live before a particular date or time, then it must be hidden with feature flags or controlled via the CMS. Simply merging code (or updating a dependency) must not result in such things being revealed.\n\n\n\n\nNew build approach\n\n\nFor projects being developed outside of cfgov-refresh\n, the relationship with the project (and particularly the \u2018build\u2019 server) is changing. Under the old system, we maintained a separate \u2018requirements\u2019 file for build, content, and production. The \u2018build\u2019 requirements generally grabbed the master branch of the app, and we would pin a particular version for content and production.\n\n\nWhat we want to do now is quite a bit different. These apps will be treated like any other python dependency (ie, always pinned to a particular version). The \u2018build\u2019 server will reflect the master branch of cfgov-refresh, but for all other projects will only reflect the current pinned version reflected in requirements.txt, and changing that version requires a pull request. We then move complete releases of the site through the QA and deployment process described below under \u201crelease cadence\u201d\n\n\nIn short: if your code is being maintained outside of cfgov-refresh, you will need to provide your own \u2018build\u2019 environment and CI pipeline. We are working with the delivery team to make this less painful than it sounds.", 
            "title": "What's new"
        }, 
        {
            "location": "/whats-new/#whats-new", 
            "text": "", 
            "title": "What\u2019s new?"
        }, 
        {
            "location": "/whats-new/#guiding-principles", 
            "text": "Reverse the sprawl of technologies and repositories left by the last 5 years of consumerfinance.gov development.   Reduce risk by integrating earlier and implementing a single release cadence for the entire site.  We don\u2019t deploy code to coincide with announcements, and events.   Prefer using and/or extending the CMS and it\u2019s primitives (page types, atomic design elements, etc) over the creation of \u201capps\u201d that own a particular URL space.", 
            "title": "Guiding principles"
        }, 
        {
            "location": "/whats-new/#benefits", 
            "text": "By centering most work on the cfgov-refresh, it becomes easier to understand what\u2019s happening, easier to communicate about the state of the site, easier to develop on, and simpler to deploy.  By working with the CMS, we empower editors to determine when a particular page or section goes live, and can provide a way to edit the incidental text on a page.", 
            "title": "Benefits"
        }, 
        {
            "location": "/whats-new/#practical-impacts", 
            "text": "Any new work that will appear on consumerfinance.gov should be built with Django, and (subject to guidelines below) live in the primary code repo for the site (cfgov-refresh).   If a particular page, section, or feature can not go live before a particular date or time, then it must be hidden with feature flags or controlled via the CMS. Simply merging code (or updating a dependency) must not result in such things being revealed.", 
            "title": "Practical impacts"
        }, 
        {
            "location": "/whats-new/#new-build-approach", 
            "text": "For projects being developed outside of cfgov-refresh , the relationship with the project (and particularly the \u2018build\u2019 server) is changing. Under the old system, we maintained a separate \u2018requirements\u2019 file for build, content, and production. The \u2018build\u2019 requirements generally grabbed the master branch of the app, and we would pin a particular version for content and production.  What we want to do now is quite a bit different. These apps will be treated like any other python dependency (ie, always pinned to a particular version). The \u2018build\u2019 server will reflect the master branch of cfgov-refresh, but for all other projects will only reflect the current pinned version reflected in requirements.txt, and changing that version requires a pull request. We then move complete releases of the site through the QA and deployment process described below under \u201crelease cadence\u201d  In short: if your code is being maintained outside of cfgov-refresh, you will need to provide your own \u2018build\u2019 environment and CI pipeline. We are working with the delivery team to make this less painful than it sounds.", 
            "title": "New build approach"
        }, 
        {
            "location": "/new-projects/", 
            "text": "Setting up new projects\n\n\nAll new code should start in the \ncfgov-refresh repository\n unless the following is true:\n\n\n\n\nIt does not require integration with CMS\n\n\nIt is not expected to match the look and feel of the larger site (in fact \"very different\" is preferable to \"almost the same\")\n\n\nIt must be pip installable like any other dependency. \n\n\n\n\nIf a project meets this criteria, it is important to note that while the app itself is not necessarily tied to the cf.gov platform\u2019s release cadence, its dependencies are. Such projects must also maintain it's own continuous integration pipeline and build server.\n\n\nFor everything else, all code in the master branch is subject to a regular release cadence. Features that must go live on a certain date should be hidden by feature flags. Deployments should not be timed to coincide with announcements, press releases, speeches, or other events. The code should be already deployed and waiting for the feature to be turned on by a site manager. See \u201cFeature Flags\u201d.\n\n\nRather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. See \u201cWagtail Pages\u201d.\n\n\nDecision Matrix\n\n\n\n\n\n\n\n\nProduct\n\n\nContent Pages\n\n\nAPIs\n\n\nHTML5 API Clients (\"single page apps\")\n\n\nTraditional Web Apps\n\n\n\n\n\n\n\n\n\n\nWhat to build\n\n\nWagtail page types and templates\n\n\nDjango app using Django REST Framework\n\n\nThe API (if needed) and Wagtail page type and template to host the client\n\n\nA standard models / forms / views-based Django app. Often calling internal/external APIs\n\n\n\n\n\n\nWhere does the code live\n\n\ncfgov-refresh\n\n\ncfgov-refresh (see exceptions)\n\n\ncfgov-refresh\n\n\ncfgov-refresh (see exceptions)\n\n\n\n\n\n\nHow to start\n\n\nExtend our existing library of page types and molecules\n\n\ncreate a Django app in the \"api's\" namespace\n\n\nBuild the API first, then create a wagtail page to host the tool\n\n\nCreate a Django app at the top level of the repo. When feasible, consider providing Wagtail page-types instead of traditional views\n\n\n\n\n\n\nHow to ship\n\n\nWith release cycle\n\n\nWith release cycle\n\n\nConsider getting APIs deployed well in advance\n\n\nwith release cycle\n\n\n\n\n\n\nHow to change\n\n\nSee below\n\n\nWith the release cycle\n\n\nUse \nAPI versioning\n to avoid breaking existing code\n\n\nSee below\n\n\n\n\n\n\n\n\nHow to handle Wagtail changes\n\n\nMinor visual updates to a page\n\n\n\n\nmake the changes as part of the release cadence\n\n\n\n\nExtensive visual changes to a page\n\n\n\n\ncreate a new template to reflect the new design\n\n\nedit the page type to allow for switching between old and new templates.\n\n\non build and staging servers, feel free to switch back and forth between the designs\n\n\n\n\nVisual and minor data model changes\n\n\n\n\nmake the data model changes in such a way that doesn't break the current template\n\n\nfollow the \"extensive visual changes to a page\" guidance above to make the visual changes\n\n\n\n\nMajor data model changes\n\n\n\n\ncreate a new page type, and the corresponding template\n\n\nEditors can then replace the old page with the new one, at will\n\n\n\n\nFront-end Resources\n\n\nFront-end resources should conform to the \nCFPB front-end guides\n and should use atomic elements, organisms, existing structure and convention. When applicable, front-end components should be added to Capital Framework using atomic design principles.\n\n\nProjects that will be part of cfgov but live in their own repositories should:\n\n\n\n\nfollow file-naming conventions for their front-end resources to avoid collisions (eg, project_name.js rather than main.js) OR follow resource folder structure conventions\n\n\nwhere possible, follow front-end guidelines/templates for new projects, including build processes and testing setup\n\n\nwhere necessary, follow a suggested approach for sharing resources (JavaScript/CSS) that are internal to cfgov-refresh", 
            "title": "New projects"
        }, 
        {
            "location": "/new-projects/#setting-up-new-projects", 
            "text": "All new code should start in the  cfgov-refresh repository  unless the following is true:   It does not require integration with CMS  It is not expected to match the look and feel of the larger site (in fact \"very different\" is preferable to \"almost the same\")  It must be pip installable like any other dependency.    If a project meets this criteria, it is important to note that while the app itself is not necessarily tied to the cf.gov platform\u2019s release cadence, its dependencies are. Such projects must also maintain it's own continuous integration pipeline and build server.  For everything else, all code in the master branch is subject to a regular release cadence. Features that must go live on a certain date should be hidden by feature flags. Deployments should not be timed to coincide with announcements, press releases, speeches, or other events. The code should be already deployed and waiting for the feature to be turned on by a site manager. See \u201cFeature Flags\u201d.  Rather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. See \u201cWagtail Pages\u201d.", 
            "title": "Setting up new projects"
        }, 
        {
            "location": "/new-projects/#decision-matrix", 
            "text": "Product  Content Pages  APIs  HTML5 API Clients (\"single page apps\")  Traditional Web Apps      What to build  Wagtail page types and templates  Django app using Django REST Framework  The API (if needed) and Wagtail page type and template to host the client  A standard models / forms / views-based Django app. Often calling internal/external APIs    Where does the code live  cfgov-refresh  cfgov-refresh (see exceptions)  cfgov-refresh  cfgov-refresh (see exceptions)    How to start  Extend our existing library of page types and molecules  create a Django app in the \"api's\" namespace  Build the API first, then create a wagtail page to host the tool  Create a Django app at the top level of the repo. When feasible, consider providing Wagtail page-types instead of traditional views    How to ship  With release cycle  With release cycle  Consider getting APIs deployed well in advance  with release cycle    How to change  See below  With the release cycle  Use  API versioning  to avoid breaking existing code  See below", 
            "title": "Decision Matrix"
        }, 
        {
            "location": "/new-projects/#how-to-handle-wagtail-changes", 
            "text": "", 
            "title": "How to handle Wagtail changes"
        }, 
        {
            "location": "/new-projects/#minor-visual-updates-to-a-page", 
            "text": "make the changes as part of the release cadence", 
            "title": "Minor visual updates to a page"
        }, 
        {
            "location": "/new-projects/#extensive-visual-changes-to-a-page", 
            "text": "create a new template to reflect the new design  edit the page type to allow for switching between old and new templates.  on build and staging servers, feel free to switch back and forth between the designs", 
            "title": "Extensive visual changes to a page"
        }, 
        {
            "location": "/new-projects/#visual-and-minor-data-model-changes", 
            "text": "make the data model changes in such a way that doesn't break the current template  follow the \"extensive visual changes to a page\" guidance above to make the visual changes", 
            "title": "Visual and minor data model changes"
        }, 
        {
            "location": "/new-projects/#major-data-model-changes", 
            "text": "create a new page type, and the corresponding template  Editors can then replace the old page with the new one, at will", 
            "title": "Major data model changes"
        }, 
        {
            "location": "/new-projects/#front-end-resources", 
            "text": "Front-end resources should conform to the  CFPB front-end guides  and should use atomic elements, organisms, existing structure and convention. When applicable, front-end components should be added to Capital Framework using atomic design principles.  Projects that will be part of cfgov but live in their own repositories should:   follow file-naming conventions for their front-end resources to avoid collisions (eg, project_name.js rather than main.js) OR follow resource folder structure conventions  where possible, follow front-end guidelines/templates for new projects, including build processes and testing setup  where necessary, follow a suggested approach for sharing resources (JavaScript/CSS) that are internal to cfgov-refresh", 
            "title": "Front-end Resources"
        }, 
        {
            "location": "/branching-merging/", 
            "text": "Branching and merging\n\n\nBranches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master. \n\n\nAll pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate.\n\n\nWhen reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review.\n\n\nThe cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass.\n\n\nPull requests that are \nnot\n to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such.\n\n\nGenerally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.", 
            "title": "Branching and merging"
        }, 
        {
            "location": "/branching-merging/#branching-and-merging", 
            "text": "Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master.   All pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate.  When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review.  The cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass.  Pull requests that are  not  to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such.  Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.", 
            "title": "Branching and merging"
        }, 
        {
            "location": "/release-cadence/", 
            "text": "Release Cadence\n\n\ncf.gov is on a two-week release cadence. The release process is as follows:\n\n\n\n\nA new branch is created from the master branch representing the next minor release, e.g. 5.1. \n\n\nA new release is tagged from that branch with a 0 patch number, e.g. 5.1.0.\n\n\nThat release is deployed to our beta server (\"beta\").\n\n\nIf any fixes are necessary before going to production, they are committed to the release branch, and back-merged to master. New \"hotfix\" releases are tagged from the branch with the appropriate patch number as needed to get urgent changes onto beta.\n\n\nThe latest release tagged on the release branch is deployed to production servers (\"content\" and \"www\"). \n\n\nIf any urgent changes are needed before the next release is deployed to www and content we follow the same \"hotfix\" procedure detailed above for beta.\n\n\n\n\n\n\nSample schedule\n\n\n\n\n\n\n\n\nMonday\n\n\nTuesday\n\n\nWednesday\n\n\nThursday\n\n\nFriday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1 branched, 5.1.0 released, deployed to beta\n\n\nHotfix 5.1.1 committed, released, deployed to beta\n\n\n\n\n\n\n\n\n5.1.1 deployed to www and content\n\n\nHotfix 5.1.2 committed, released, deployed to www and content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 branched, 5.2.0 released, deployed to beta", 
            "title": "Release cadence"
        }, 
        {
            "location": "/release-cadence/#release-cadence", 
            "text": "cf.gov is on a two-week release cadence. The release process is as follows:   A new branch is created from the master branch representing the next minor release, e.g. 5.1.   A new release is tagged from that branch with a 0 patch number, e.g. 5.1.0.  That release is deployed to our beta server (\"beta\").  If any fixes are necessary before going to production, they are committed to the release branch, and back-merged to master. New \"hotfix\" releases are tagged from the branch with the appropriate patch number as needed to get urgent changes onto beta.  The latest release tagged on the release branch is deployed to production servers (\"content\" and \"www\").   If any urgent changes are needed before the next release is deployed to www and content we follow the same \"hotfix\" procedure detailed above for beta.", 
            "title": "Release Cadence"
        }, 
        {
            "location": "/release-cadence/#sample-schedule", 
            "text": "Monday  Tuesday  Wednesday  Thursday  Friday         5.1 branched, 5.1.0 released, deployed to beta  Hotfix 5.1.1 committed, released, deployed to beta     5.1.1 deployed to www and content  Hotfix 5.1.2 committed, released, deployed to www and content         5.2 branched, 5.2.0 released, deployed to beta", 
            "title": "Sample schedule"
        }, 
        {
            "location": "/deployment/", 
            "text": "Deployment\n\n\nEverything that is part of cfgov-refresh and its dependencies are deployed as part of the cf.gov deployment jobs. For needs that are outside of the standard deployment process (perhaps relating to regular data loading or manipulation), additional jobs will need to be created.\n\n\nProjects that are independent of cfgov-refresh will need to provide their own deployment process, and all jobs they require. They will not be automatically included in the cf.gov deployment process, and will not be on the cf.gov release cadence. \n\n\nAll new deployment jobs, jobs that run in addition to the cf.gov deployment jobs, as well as independent jobs, must be implemented with \nJenkins-as-code\n.\n\n\nDeployment QA\n\n\nAll code that gets merged into the cfgov-refresh master must have adequate tests, as appropriate for the nature of the code. \n\n\nThis could potentially include unit tests, browser tests, and 508-compliance tests.\n\n\nThere should be no drop in test coverage for cfgov-refresh.\n\n\nRegular releases of cf.gov on our release cadence are automated, presuming:\n\n\n\n\nAll unit tests pass \n\n\nAll functional tests pass\n\n\nThere is no reduction in test coverage from the last release tagging\n\n\n\n\nCurrent deployment process\n\n\nThe current cf.gov deployment process requires running the cf.gov-pipeline-build Jenkins pipeline job, with the release tag to deploy and the environment to which it should be deployed.\n\n\nThe pipeline will invoke the following jobs, in order:\n\n\n\n\ncf.gov-frontend-build, which builds the front-end assets for cfgov-refresh\n\n\ncf.gov-deploy-django, which takes the cfgov-refresh build, and uses \ndrama-free-django\n to build a deployable artifact for all of cf.gov and its requirements, and then deploys the artifact to the appropriate servers for the selected environment.\n\n\n\n\nAny back-end developer on the platform team should be able to assist with deployments. You may also contact the following individually:\n\n\n\n\nRoss Karchner\n\n\nScott Cranfill\n\n\nWill Barton\n\n\nBill Higgins\n\n\nSerghei Gorobet\n\n\nThe Software Delivery team", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment/#deployment", 
            "text": "Everything that is part of cfgov-refresh and its dependencies are deployed as part of the cf.gov deployment jobs. For needs that are outside of the standard deployment process (perhaps relating to regular data loading or manipulation), additional jobs will need to be created.  Projects that are independent of cfgov-refresh will need to provide their own deployment process, and all jobs they require. They will not be automatically included in the cf.gov deployment process, and will not be on the cf.gov release cadence.   All new deployment jobs, jobs that run in addition to the cf.gov deployment jobs, as well as independent jobs, must be implemented with  Jenkins-as-code .", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment/#deployment-qa", 
            "text": "All code that gets merged into the cfgov-refresh master must have adequate tests, as appropriate for the nature of the code.   This could potentially include unit tests, browser tests, and 508-compliance tests.  There should be no drop in test coverage for cfgov-refresh.  Regular releases of cf.gov on our release cadence are automated, presuming:   All unit tests pass   All functional tests pass  There is no reduction in test coverage from the last release tagging", 
            "title": "Deployment QA"
        }, 
        {
            "location": "/deployment/#current-deployment-process", 
            "text": "The current cf.gov deployment process requires running the cf.gov-pipeline-build Jenkins pipeline job, with the release tag to deploy and the environment to which it should be deployed.  The pipeline will invoke the following jobs, in order:   cf.gov-frontend-build, which builds the front-end assets for cfgov-refresh  cf.gov-deploy-django, which takes the cfgov-refresh build, and uses  drama-free-django  to build a deployable artifact for all of cf.gov and its requirements, and then deploys the artifact to the appropriate servers for the selected environment.   Any back-end developer on the platform team should be able to assist with deployments. You may also contact the following individually:   Ross Karchner  Scott Cranfill  Will Barton  Bill Higgins  Serghei Gorobet  The Software Delivery team", 
            "title": "Current deployment process"
        }, 
        {
            "location": "/feature-flags/", 
            "text": "Feature flags\n\n\nFeature flags are implemented using our \nWagtail-Flags\n app. The \nREADME\n contains an overview and examples of how to use feature flags in Wagtail.\n\n\nThis document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use.\n\n\n\n\nAdding a flag\n\n\nChecking a flag\n\n\nIn templates\n\n\nJinja2\n\n\nDjango\n\n\n\n\n\n\nIn code\n\n\nIn URLs\n\n\n\n\n\n\nEnabling a flag\n\n\nHard-coded conditions\n\n\nDatabase conditions\n\n\n\n\n\n\nSatellite apps\n\n\nHygiene\n\n\n\n\nAdding a flag\n\n\nFeature flags are defined in code in the \ncfgov/settings/base.py\n file as part of the \nFLAGS\n setting. Each flag consists of a single string and a Python dictionary (\n{}\n) of its hard-coded conditions (see \nEnabling a flag\n below).\n\n\nFLAGS\n \n=\n \n{\n\n    \n# Beta banner, seen on beta.consumerfinance.gov\n\n    \n# When enabled, a banner appears across the top of the site proclaiming\n\n    \n# \nThis beta site is a work in progress.\n\n    \nBETA_NOTICE\n:\n \n{},\n\n\n}\n\n\n\n\n\n\nBy convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.\n\n\nChecking a flag\n\n\nFlags can be checked either in Python code or in Django or Jinja2 template files. See the full \nWagtail Flags API is documented \n for more information.\n\n\nIn templates\n\n\nJinja2\n\n\nMost of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided, \nflag_enabled\n and \nflag_disabled\n. Each takes a \nrequest\n object as its first argument and flag name as the second.\n\n\nflag_enabled(request, 'MY_FLAG')\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare\n met.\n\n\nflag_disabled(request, 'MY_FLAG')\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare not\n met.\n\n\nSee \nEnabling a flag\n below for more on flag conditions.\n\n\nAn example is \nthe \nBETA_NOTICE flag\n as implemented in \nheader.html\n:\n\n\n{%\n \nif\n \nflag_enabled\n(\nrequest\n,\n \nBETA_NOTICE\n)\n \nand\n \nshow_banner\n \n%}\n\n\ndiv class=\nm-global-banner\n\n\n    \ndiv class=\nwrapper\n\n\n                wrapper__match-content\n\n\n                o-expandable\n\n\n                o-expandable__expanded\n\n\n        \ndiv class=\nm-global-banner_head\n\n\n            \nspan class=\ncf-icon\n\n\n                         cf-icon-error-round\n\n\n                         m-global-banner_icon\n/span\n\n\n            This beta site is a work in progress.\n\n\n        \n/div\n\n\n        \u2026\n\n\n    \n/div\n\n\n/div\n\n\n{%\n \nendif\n \n%}\n\n\n\n\n\n\nDjango\n\n\nIn Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided \nflag_enabled\n and \nflag_disabled\n once the \nfeature_flags\n template tag library is loaded.\n\n\nflag_enabled 'MY_FLAG'\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare\n met.\n\n\nflag_disabled 'MY_FLAG'\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare not\n met.\n\n\nSee \nEnabling a flag\n below for more on flag conditions.\n\n\nThe \nBETA_NOTICE\n \nJinja2\n example above when implemented with Django templates would look like this:\n\n\n{%\n \nload\n \nfeature_flags\n \n%}\n\n\n\n{%\n \nflag_enabled\n \nBETA_NOTICE\n \nas\n \nbeta_flag\n \n%}\n\n\n{%\n \nif\n \nbeta_flag\n \nand\n \nshow_banner\n \n%}\n\n\ndiv class=\nm-global-banner\n\n\n    \ndiv class=\nwrapper\n\n\n                wrapper__match-content\n\n\n                o-expandable\n\n\n                o-expandable__expanded\n\n\n        \ndiv class=\nm-global-banner_head\n\n\n            \nspan class=\ncf-icon\n\n\n                         cf-icon-error-round\n\n\n                         m-global-banner_icon\n/span\n\n\n            This beta site is a work in progress.\n\n\n        \n/div\n\n\n        \u2026\n\n\n    \n/div\n\n\n/div\n\n\n{%\n \nendif\n \n%}\n\n\n\n\n\n\nIn code\n\n\nIn Python code three functions are available for checking feature flags, \nflag_state\n, \nflag_enabled\n, and \nflag_disabled\n. The Python API is slightly different from the \nJinja2\n or \nDjango template\n API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle).\n\n\nSee the \nWagtail Flags flag state API documentation for more\n.\n\n\nAdditionally two decorators, \nflag_check\n and \nflag_required\n, are provided for wrapping views (and another functions) in a feature flag check.  See the \nWagtail Flags flag decorators API documentation for more\n.\n\n\nIn URLs\n\n\nThere are two ways to flag Django URL patterns in \nurls.py\n: with \nflagged_url()\n in place of \nurl()\n for a single pattern, or with the \nflagged_urls()\n context manager for multiple URLs.\n\n\nflagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None)\n works exactly like \nurl()\n except it takes a flag name as its first argument. If the flag's state matches the given \nstate\n, the URL pattern will be served from the given \nview\n; if not, and \nfallback\n is given, the \nfallback\n will be used.\n\n\nAn example is \nour \nWAGTAIL_ABOUT_US\n flag\n:\n\n\nflagged_url\n(\nWAGTAIL_ABOUT_US\n,\n\n            \nr\n^about-us/$\n,\n\n            \nlambda\n \nreq\n:\n \nServeView\n.\nas_view\n()(\nreq\n,\n \nreq\n.\npath\n),\n\n            \nfallback\n=\nSheerTemplateView\n.\nas_view\n(\n\n                \ntemplate_name\n=\nabout-us/index.html\n),\n\n            \nname\n=\nabout-us\n),\n\n\n\n\n\n\nIgnoring the \nview\n being a \nlambda\n for now (see \nFlagging Wagtail URLs below\n), this URL will be served via Wagtail if \nWAGTAIL_ABOUT_US\n's conditions are \nTrue\n, and from a \nTemplateView\n if its conditions are \nFalse\n.\n\n\nIf you need to flag multiple URLs with the same flag, you can use the \nflagged_urls()\n context manager.\n\n\nwith flagged_urls(flag_name, state=True, fallback=None) as url\n provides a context in which the returned \nurl()\n function can be used in place of the Django \nurl()\n function in patterns and those patterns will share the same feature flag, state, and fallback.\n\n\nAn example is \nour \nWAGTAIL_ASK_CFPB\n flag\n:\n\n\nwith\n \nflagged_urls\n(\nWAGTAIL_ASK_CFPB\n)\n \nas\n \nurl\n:\n\n    \nask_patterns\n \n=\n \n[\n\n        \nurl\n(\nr\n^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$\n,\n\n            \nview_answer\n,\n\n            \nname\n=\nask-english-answer\n),\n\n        \nurl\n(\nr\n^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$\n,\n\n            \nview_answer\n,\n\n            \nname\n=\nask-spanish-answer\n),\n\n        \n\u2026\n\n    \n]\n\n\n\nurlpatterns\n \n+=\n \nask_patterns\n\n\n\n\n\n\n\n\nWarning\n\n\nDo not attempt to use \nflag_check\n or any flag state-checking functions in \nurls.py\n. Because they will be evaluated on import of \nurls.py\n they will attempt to access the Django FlagState model before it is ready and will error.\n\n\n\n\nFlagging Wagtail URLs\n\n\nWagtail views in \nflagged_url\n with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with \nrequest\n as the first argument, and Wagtail's \nserve\n view takes both the request and the path. To get around this, in \nflagged_url\n we typically use a \nlambda\n for the view:\n\n\nlambda\n \nreq\n:\n \nServeView\n.\nas_view\n()(\nreq\n,\n \nreq\n.\npath\n)\n\n\n\n\n\n\nThis lambda takes the request and calls the \nWagtail-Sharing\n \nServeView\n (which we're using in place of \nwagtail.wagtailcore.views.serve\n).\n\n\nEnabling a flag\n\n\nFeature flags are enabled based on a set of conditions that are given either in the Django settings files (in \ncfgov/cfgov/settings/\n) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled.\n\n\nA list of available conditions and how to use them is available in the Wagtail-Flags documentation\n.\n\n\nHard-coded conditions\n\n\nConditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change.\n\n\nWhen \nadding a flag\n to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. \nSee the Wagtail-Flags conditions documentation\n for more on individual conditions.\n\n\nThere is a simple \nboolean\n condition that is either \nTrue\n or \nFalse\n, and if it is \nTrue\n the flag is enabled and if it is \nFalse\n the flag is disabled. If we want to always turn the \nBETA_NOTICE\n flag on in settings with a \nboolean\n condition, that would look like this:\n\n\nFLAGS\n \n=\n \n{\n\n    \n# Beta banner, seen on beta.consumerfinance.gov\n\n    \n# When enabled, a banner appears across the top of the site proclaiming\n\n    \n# \nThis beta site is a work in progress.\n\n    \nBETA_NOTICE\n:\n \n{\n\n        \nboolean\n:\n \nTrue\n,\n\n    \n},\n\n\n}\n\n\n\n\n\n\nDatabase conditions\n\n\nConditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place).\n\n\nTo view, delete, and add database conditions, navigate to \"Settings \n Flags\" in the Wagtail admin.\n\n\n\n\nOnce in the flag settings, you'll have a list of all flags and their conditions..\n\n\n\n\nDatabase conditions can be deleted with the trash can button on the right.\n\n\nTo create a new database condition, select \"Add a condition\". As with \nhard-coded conditions\n, to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled.\n\n\n\n\nDatabase conditions can only be set for flags that exist in the Django settings.\n\n\nSatellite apps\n\n\nFeature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is \nthe use of a feature flagged template choice in the complaintdatabase app\n.\n\n\nHygiene\n\n\nFeature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.", 
            "title": "Feature flags"
        }, 
        {
            "location": "/feature-flags/#feature-flags", 
            "text": "Feature flags are implemented using our  Wagtail-Flags  app. The  README  contains an overview and examples of how to use feature flags in Wagtail.  This document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use.   Adding a flag  Checking a flag  In templates  Jinja2  Django    In code  In URLs    Enabling a flag  Hard-coded conditions  Database conditions    Satellite apps  Hygiene", 
            "title": "Feature flags"
        }, 
        {
            "location": "/feature-flags/#adding-a-flag", 
            "text": "Feature flags are defined in code in the  cfgov/settings/base.py  file as part of the  FLAGS  setting. Each flag consists of a single string and a Python dictionary ( {} ) of its hard-coded conditions (see  Enabling a flag  below).  FLAGS   =   { \n     # Beta banner, seen on beta.consumerfinance.gov \n     # When enabled, a banner appears across the top of the site proclaiming \n     #  This beta site is a work in progress. \n     BETA_NOTICE :   {},  }   By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.", 
            "title": "Adding a flag"
        }, 
        {
            "location": "/feature-flags/#checking-a-flag", 
            "text": "Flags can be checked either in Python code or in Django or Jinja2 template files. See the full  Wagtail Flags API is documented   for more information.", 
            "title": "Checking a flag"
        }, 
        {
            "location": "/feature-flags/#in-templates", 
            "text": "", 
            "title": "In templates"
        }, 
        {
            "location": "/feature-flags/#jinja2", 
            "text": "Most of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided,  flag_enabled  and  flag_disabled . Each takes a  request  object as its first argument and flag name as the second.  flag_enabled(request, 'MY_FLAG')  will return  True  if the conditions under which  MY_FLAG  is enabled  are  met.  flag_disabled(request, 'MY_FLAG')  will return  True  if the conditions under which  MY_FLAG  is enabled  are not  met.  See  Enabling a flag  below for more on flag conditions.  An example is  the  BETA_NOTICE flag  as implemented in  header.html :  {%   if   flag_enabled ( request ,   BETA_NOTICE )   and   show_banner   %}  div class= m-global-banner       div class= wrapper                  wrapper__match-content                  o-expandable                  o-expandable__expanded           div class= m-global-banner_head               span class= cf-icon                           cf-icon-error-round                           m-global-banner_icon /span              This beta site is a work in progress.           /div          \u2026       /div  /div  {%   endif   %}", 
            "title": "Jinja2"
        }, 
        {
            "location": "/feature-flags/#django", 
            "text": "In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided  flag_enabled  and  flag_disabled  once the  feature_flags  template tag library is loaded.  flag_enabled 'MY_FLAG'  will return  True  if the conditions under which  MY_FLAG  is enabled  are  met.  flag_disabled 'MY_FLAG'  will return  True  if the conditions under which  MY_FLAG  is enabled  are not  met.  See  Enabling a flag  below for more on flag conditions.  The  BETA_NOTICE   Jinja2  example above when implemented with Django templates would look like this:  {%   load   feature_flags   %}  {%   flag_enabled   BETA_NOTICE   as   beta_flag   %}  {%   if   beta_flag   and   show_banner   %}  div class= m-global-banner       div class= wrapper                  wrapper__match-content                  o-expandable                  o-expandable__expanded           div class= m-global-banner_head               span class= cf-icon                           cf-icon-error-round                           m-global-banner_icon /span              This beta site is a work in progress.           /div          \u2026       /div  /div  {%   endif   %}", 
            "title": "Django"
        }, 
        {
            "location": "/feature-flags/#in-code", 
            "text": "In Python code three functions are available for checking feature flags,  flag_state ,  flag_enabled , and  flag_disabled . The Python API is slightly different from the  Jinja2  or  Django template  API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle).  See the  Wagtail Flags flag state API documentation for more .  Additionally two decorators,  flag_check  and  flag_required , are provided for wrapping views (and another functions) in a feature flag check.  See the  Wagtail Flags flag decorators API documentation for more .", 
            "title": "In code"
        }, 
        {
            "location": "/feature-flags/#in-urls", 
            "text": "There are two ways to flag Django URL patterns in  urls.py : with  flagged_url()  in place of  url()  for a single pattern, or with the  flagged_urls()  context manager for multiple URLs.  flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None)  works exactly like  url()  except it takes a flag name as its first argument. If the flag's state matches the given  state , the URL pattern will be served from the given  view ; if not, and  fallback  is given, the  fallback  will be used.  An example is  our  WAGTAIL_ABOUT_US  flag :  flagged_url ( WAGTAIL_ABOUT_US , \n             r ^about-us/$ , \n             lambda   req :   ServeView . as_view ()( req ,   req . path ), \n             fallback = SheerTemplateView . as_view ( \n                 template_name = about-us/index.html ), \n             name = about-us ),   Ignoring the  view  being a  lambda  for now (see  Flagging Wagtail URLs below ), this URL will be served via Wagtail if  WAGTAIL_ABOUT_US 's conditions are  True , and from a  TemplateView  if its conditions are  False .  If you need to flag multiple URLs with the same flag, you can use the  flagged_urls()  context manager.  with flagged_urls(flag_name, state=True, fallback=None) as url  provides a context in which the returned  url()  function can be used in place of the Django  url()  function in patterns and those patterns will share the same feature flag, state, and fallback.  An example is  our  WAGTAIL_ASK_CFPB  flag :  with   flagged_urls ( WAGTAIL_ASK_CFPB )   as   url : \n     ask_patterns   =   [ \n         url ( r ^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$ , \n             view_answer , \n             name = ask-english-answer ), \n         url ( r ^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$ , \n             view_answer , \n             name = ask-spanish-answer ), \n         \u2026 \n     ]  urlpatterns   +=   ask_patterns    Warning  Do not attempt to use  flag_check  or any flag state-checking functions in  urls.py . Because they will be evaluated on import of  urls.py  they will attempt to access the Django FlagState model before it is ready and will error.", 
            "title": "In URLs"
        }, 
        {
            "location": "/feature-flags/#flagging-wagtail-urls", 
            "text": "Wagtail views in  flagged_url  with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with  request  as the first argument, and Wagtail's  serve  view takes both the request and the path. To get around this, in  flagged_url  we typically use a  lambda  for the view:  lambda   req :   ServeView . as_view ()( req ,   req . path )   This lambda takes the request and calls the  Wagtail-Sharing   ServeView  (which we're using in place of  wagtail.wagtailcore.views.serve ).", 
            "title": "Flagging Wagtail URLs"
        }, 
        {
            "location": "/feature-flags/#enabling-a-flag", 
            "text": "Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in  cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled.  A list of available conditions and how to use them is available in the Wagtail-Flags documentation .", 
            "title": "Enabling a flag"
        }, 
        {
            "location": "/feature-flags/#hard-coded-conditions", 
            "text": "Conditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change.  When  adding a flag  to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type.  See the Wagtail-Flags conditions documentation  for more on individual conditions.  There is a simple  boolean  condition that is either  True  or  False , and if it is  True  the flag is enabled and if it is  False  the flag is disabled. If we want to always turn the  BETA_NOTICE  flag on in settings with a  boolean  condition, that would look like this:  FLAGS   =   { \n     # Beta banner, seen on beta.consumerfinance.gov \n     # When enabled, a banner appears across the top of the site proclaiming \n     #  This beta site is a work in progress. \n     BETA_NOTICE :   { \n         boolean :   True , \n     },  }", 
            "title": "Hard-coded conditions"
        }, 
        {
            "location": "/feature-flags/#database-conditions", 
            "text": "Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place).  To view, delete, and add database conditions, navigate to \"Settings   Flags\" in the Wagtail admin.   Once in the flag settings, you'll have a list of all flags and their conditions..   Database conditions can be deleted with the trash can button on the right.  To create a new database condition, select \"Add a condition\". As with  hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled.   Database conditions can only be set for flags that exist in the Django settings.", 
            "title": "Database conditions"
        }, 
        {
            "location": "/feature-flags/#satellite-apps", 
            "text": "Feature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is  the use of a feature flagged template choice in the complaintdatabase app .", 
            "title": "Satellite apps"
        }, 
        {
            "location": "/feature-flags/#hygiene", 
            "text": "Feature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.", 
            "title": "Hygiene"
        }, 
        {
            "location": "/wagtail-migrations/", 
            "text": "Wagtail and Django data migrations\n\n\nDjango data migrations with Wagtail can be interesting and challenging because programmatic editing of Wagtail pages \nis difficult\n, and pages have both revisions and StreamFields. This document is intended to describe ways we try to address these challenges in cfgov-refresh.\n\n\nMigrating StreamFields\n\n\nStreamFields do not follow a fixed structure, rather they're a freeform sequences of blocks. Making a change to a StreamField involves both creating a \nDjango schema migration\n and a custom \nDjango data migration\n. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the StreamField contents. \n\n\nTo this end, there are some utility functions in cfgov-refresh that make this easier. Using these utilities, a Django data migration that modifies a StreamField would follow the following format:\n\n\nfrom\n \ndjango.db\n \nimport\n \nmigrations\n\n\n\nfrom\n \nv1.util.migrations\n \nimport\n \nmigrate_page_types_and_fields\n\n\n\n\ndef\n \nforward_mapper\n(\npage_or_revision\n,\n \ndata\n):\n\n    \ndata\n \n=\n \ndict\n(\ndata\n)\n\n    \n# Manipulate the stream block data forwards\n\n    \nreturn\n \ndata\n\n\n\n\ndef\n \nbackward_mapper\n(\npage_or_revision\n,\n \ndata\n):\n\n    \ndata\n \n=\n \ndict\n(\ndata\n)\n\n    \n# Manipulate the stream block data backwards\n\n    \nreturn\n \ndata\n\n\n\n\ndef\n \nforwards\n(\napps\n,\n \nschema_editor\n):\n\n    \npage_types_and_fields\n \n=\n \n[\n\n        \n(\nmyapp\n,\n \nMyPage\n,\n \nstreamfield_name\n,\n \nstreamblock_type\n),\n\n    \n]\n\n    \nmigrate_page_types_and_fields\n(\napps\n,\n \n                                  \npage_types_and_fields\n,\n\n                                  \nforward_mapper\n)\n\n\n\n\ndef\n \nbackwards\n(\napps\n,\n \nschema_editor\n):\n\n    \npage_types_and_fields\n \n=\n \n[\n\n        \n(\nmyapp\n,\n \nMyPage\n,\n \nstreamfield_name\n,\n \nstreamblock_type\n),\n\n    \n]\n\n    \nmigrate_page_types_and_fields\n(\napps\n,\n \n                                  \npage_types_and_fields\n,\n\n                                  \nbackward_mapper\n)\n\n\n\n\nclass\n \nMigration\n(\nmigrations\n.\nMigration\n):\n\n    \ndependencies\n \n=\n \n[]\n\n    \noperations\n \n=\n \n[\n\n        \nmigrations\n.\nRunPython\n(\nforwards\n,\n \nbackwards\n),\n\n    \n]\n\n\n\n\n\n\nUtility functions\n\n\nThese functions are available in \nv1.util.migrations\n.\n\n\nmigrate_page_types_and_fields(apps, page_types_and_fields, mapper)\n\n\nMigrate the fields of a wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block type').\n\n\nThe mapper function should take \npage_or_revision\n and the stream block value.\n\n\nmigrate_stream_field(page_or_revision, field_name, block_type, mapper)\n\n\nMigrate a block of the type within a StreamField of the name belonging to the page or revision using the mapper function.\n\n\nThe mapper function should take \npage_or_revision\n and the stream block value.\n\n\nget_stream_data(page_or_revision, field_name)\n\n\nGet the stream field data for a given field name on a page or a revision.\n\n\nThis function will return a list of \ndict\n-like objects containing the blocks within the given StreamField.\n\n\nset_stream_data(page_or_revision, field_name, stream_data, commit=True)\n\n\nSet the stream field data for a given field name on a page or a revision. If commit is True (default) \nsave()\n is called on the \npage_or_revision\n object. \n\n\nstream_data\n must be a list of \ndict\n-like objects containing the blocks within the given StreamField.", 
            "title": "Wagtail and Django migrations"
        }, 
        {
            "location": "/wagtail-migrations/#wagtail-and-django-data-migrations", 
            "text": "Django data migrations with Wagtail can be interesting and challenging because programmatic editing of Wagtail pages  is difficult , and pages have both revisions and StreamFields. This document is intended to describe ways we try to address these challenges in cfgov-refresh.", 
            "title": "Wagtail and Django data migrations"
        }, 
        {
            "location": "/wagtail-migrations/#migrating-streamfields", 
            "text": "StreamFields do not follow a fixed structure, rather they're a freeform sequences of blocks. Making a change to a StreamField involves both creating a  Django schema migration  and a custom  Django data migration . The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the StreamField contents.   To this end, there are some utility functions in cfgov-refresh that make this easier. Using these utilities, a Django data migration that modifies a StreamField would follow the following format:  from   django.db   import   migrations  from   v1.util.migrations   import   migrate_page_types_and_fields  def   forward_mapper ( page_or_revision ,   data ): \n     data   =   dict ( data ) \n     # Manipulate the stream block data forwards \n     return   data  def   backward_mapper ( page_or_revision ,   data ): \n     data   =   dict ( data ) \n     # Manipulate the stream block data backwards \n     return   data  def   forwards ( apps ,   schema_editor ): \n     page_types_and_fields   =   [ \n         ( myapp ,   MyPage ,   streamfield_name ,   streamblock_type ), \n     ] \n     migrate_page_types_and_fields ( apps ,  \n                                   page_types_and_fields , \n                                   forward_mapper )  def   backwards ( apps ,   schema_editor ): \n     page_types_and_fields   =   [ \n         ( myapp ,   MyPage ,   streamfield_name ,   streamblock_type ), \n     ] \n     migrate_page_types_and_fields ( apps ,  \n                                   page_types_and_fields , \n                                   backward_mapper )  class   Migration ( migrations . Migration ): \n     dependencies   =   [] \n     operations   =   [ \n         migrations . RunPython ( forwards ,   backwards ), \n     ]", 
            "title": "Migrating StreamFields"
        }, 
        {
            "location": "/wagtail-migrations/#utility-functions", 
            "text": "These functions are available in  v1.util.migrations .", 
            "title": "Utility functions"
        }, 
        {
            "location": "/wagtail-migrations/#migrate_page_types_and_fieldsapps-page_types_and_fields-mapper", 
            "text": "Migrate the fields of a wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block type').  The mapper function should take  page_or_revision  and the stream block value.", 
            "title": "migrate_page_types_and_fields(apps, page_types_and_fields, mapper)"
        }, 
        {
            "location": "/wagtail-migrations/#migrate_stream_fieldpage_or_revision-field_name-block_type-mapper", 
            "text": "Migrate a block of the type within a StreamField of the name belonging to the page or revision using the mapper function.  The mapper function should take  page_or_revision  and the stream block value.", 
            "title": "migrate_stream_field(page_or_revision, field_name, block_type, mapper)"
        }, 
        {
            "location": "/wagtail-migrations/#get_stream_datapage_or_revision-field_name", 
            "text": "Get the stream field data for a given field name on a page or a revision.  This function will return a list of  dict -like objects containing the blocks within the given StreamField.", 
            "title": "get_stream_data(page_or_revision, field_name)"
        }, 
        {
            "location": "/wagtail-migrations/#set_stream_datapage_or_revision-field_name-stream_data-committrue", 
            "text": "Set the stream field data for a given field name on a page or a revision. If commit is True (default)  save()  is called on the  page_or_revision  object.   stream_data  must be a list of  dict -like objects containing the blocks within the given StreamField.", 
            "title": "set_stream_data(page_or_revision, field_name, stream_data, commit=True)"
        }, 
        {
            "location": "/django-to-wagtail/", 
            "text": "Wagtail pages vs. Django views\n\n\nRather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. This will allow site editors to drop that page anywhere in the site\u2019s URL structure that they wish. A Wagtail Page subclass can do anything a Django view can \nwhen overriding the serve method\n.\n\n\nfrom\n \ndjango.http\n \nimport\n \nHttpResponse\n\n\nfrom\n \nwagtail.wagtailcore.models\n \nimport\n \nPage\n,\n\n\n\nclass\n \nHelloWorldPage\n(\nCFGOVPage\n):\n\n    \ndef\n \nserve\n(\nself\n,\n \nrequest\n):\n\n        \nreturn\n \nHttpResponse\n(\nHello World\n)\n\n\n\n\n\n\nBy working with the Wagtail CMS, we also get some of the benefits of feature flags for free.", 
            "title": "Wagtail Pages vs Django views"
        }, 
        {
            "location": "/django-to-wagtail/#wagtail-pages-vs-django-views", 
            "text": "Rather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. This will allow site editors to drop that page anywhere in the site\u2019s URL structure that they wish. A Wagtail Page subclass can do anything a Django view can  when overriding the serve method .  from   django.http   import   HttpResponse  from   wagtail.wagtailcore.models   import   Page ,  class   HelloWorldPage ( CFGOVPage ): \n     def   serve ( self ,   request ): \n         return   HttpResponse ( Hello World )   By working with the Wagtail CMS, we also get some of the benefits of feature flags for free.", 
            "title": "Wagtail pages vs. Django views"
        }, 
        {
            "location": "/atomic-structure/", 
            "text": "Notes on Atomic Design\n\n\nCheck out \nDon't Build Pages, Build Modules\n. It encompasses exactly what we are trying to achieve by building components using atomic design. It's important to note that our front-end atomic architecture is still evolving.\n\n\nOur components are broken down into templates, organisms, molecules, and atoms. We opted not to use the page component, although it exists in atomic design. Our components are composed of HTML, CSS, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated js and/or css file. We compose our atomic components as follows:\n\n\nAtoms\n\n\nPrefixed with \u201ca-\u201d in CSS, JavaScript, and HTML files.\n\n\nHTML\n\n\ndiv\n \nclass\n=\na-overlay u-hidden\n/\ndiv\n\n\n\n\n\n\nCSS\n\n\n \n.\na-overlay\n \n{\n\n        \n//\n \nOnly\n \nshow\n \noverlay\n \nat\n \nmobile/tablet\n \nsize.\n\n        \n.respond-to-max(\n \n@bp-sm-max,\n \n{\n\n            \nheight\n:\n \n100\n%\n;\n\n            \nwidth\n:\n \n100\n%\n;\n\n\n\n\n\n\nMolecules\n\n\nPrefixed with \u201cm-\u201d in CSS, JavaScript, and HTML files.\n\n\nHTML\n\n\ndiv\n \nclass\n=\nm-notification m-notification__error m-notification__visible\n \ndata-js-hook\n=\nstate_atomic_init\n\n      \nspan\n \nclass\n=\nm-notification_icon cf-icon\n/\nspan\n\n      \ndiv\n \nclass\n=\nm-notification_content\n \nrole\n=\nalert\n/\ndiv\n\n \n/\ndiv\n\n\n\n\n\n\nCSS\n\n\n.\nm-notification\n \n{\n\n    \ndisplay\n:\n \nnone\n;\n\n    \nposition\n:\n \nrelative\n;\n\n    \npadding\n:\n \n@\nm-notification-padding__px\n;\n\n    \npadding-left\n:\n \n40\npx\n;\n\n\n\n\n\n\nJavaScript\n\n\nfunction\n \nNotification\n(\n \nelement\n \n)\n \n{\n\n   \n// eslint-disable-line max-statements, inline-comments, max-len\n\n   \nvar\n \nBASE_CLASS\n \n=\n \nm-notification\n;\n\n\n   \n// Constants for the state of this Notification.\n\n   \nvar\n \nSUCCESS\n \n=\n \nsuccess\n;\n\n   \nvar\n \nWARNING\n \n=\n \nwarning\n;\n\n   \nvar\n \nERROR\n \n=\n \nerror\n;\n\n   \n// Constants for the Notification modifiers.\n\n   \nvar\n \nMODIFIER_VISIBLE\n \n=\n \nBASE_CLASS\n \n+\n \n__visible\n;\n\n   \nvar\n \n_dom\n \n=\n \natomicHelpers\n.\ncheckDom\n(\n \nelement\n,\n \nBASE_CLASS\n \n);\n\n   \nvar\n \n_contentDom\n \n=\n \n_dom\n.\nquerySelector\n(\n \n.\n \n+\n \nBASE_CLASS\n \n+\n \n_content\n \n);\n\n\n\n\n\n\nThe notification molecule can be instantiated with the following code:\n\n\n_notification\n \n=\n \nnew\n \nNotification\n(\n \n_dom\n \n);\n\n\n_notification\n.\ninit\n();\n\n\n\n\n\n\nOrganisms\n\n\nPrefixed with \u201co-\u201d in CSS, JavaScript, and HTML.\n\n\nHTML\n\n\ndiv\n \ndata-qa-hook\n=\nexpandable\n \nclass\n=\no-expandable\n\n\n                                      o-expandable__borders\n\n\n                                      o-expandable__midtone\n\n\n                                      o-expandable__expanded\n\n                               \ndata-js-hook\n=\nstate_atomic_init\n\n    \nbutton\n \nclass\n=\no-expandable_target\n \naria-pressed\n=\ntrue\n\n        \ndiv\n \nclass\n=\no-expandable_header\n\n\n\n\n\n\nJavaScript:\n\n\n \nfunction\n \nExpandable\n(\n \nelement\n \n)\n \n{\n\n  \nvar\n \nBASE_CLASS\n \n=\n \no-expandable\n;\n\n\n  \n// Bitwise flags for the state of this Expandable.\n\n  \nvar\n \nCOLLAPSED\n \n=\n \n0\n;\n\n  \nvar\n \nCOLLAPSING\n \n=\n \n1\n;\n\n  \nvar\n \nEXPANDING\n \n=\n \n2\n;\n\n  \nvar\n \nEXPANDED\n \n=\n \n3\n;\n\n\n  \n// The Expandable element will directly be the Expandable\n\n  \n// when used in an ExpandableGroup, otherwise it can be the parent container.\n\n  \nvar\n \n_dom\n \n=\n \natomicHelpers\n.\ncheckDom\n(\n \nelement\n,\n \nBASE_CLASS\n \n);\n\n  \nvar\n \n_target\n \n=\n \n_dom\n.\nquerySelector\n(\n \n.\n \n+\n \nBASE_CLASS\n \n+\n \n_target\n \n);\n\n  \nvar\n \n_content\n \n=\n \n_dom\n.\nquerySelector\n(\n \n.\n \n+\n \nBASE_CLASS\n \n+\n \n_content\n \n);\n\n\n\n\n\n\nThe Expandable organism can be instantiated with the following code:\n\n\n_expandable\n \n=\n \nnew\n \nExpandable\n(\n \n_dom\n.\nquerySelector\n(\n \n.o-expandable\n \n)\n \n);\n\n\n_expandable\n.\ninit\n(\n \n_expandable\n.\nEXPANDED\n \n);\n\n\n\n\n\n\nor\n\n\nvar\n \natomicHelpers\n \n=\n \nrequire\n(\n \n../../modules/util/atomic-helpers\n \n);\n\n\nvar\n \nExpandable\n \n=\n \nrequire\n(\n \n../../organisms/Expandable\n \n);\n\n\natomicHelpers\n.\ninstantiateAll\n(\n \n.o-expandable\n,\n \nExpandable\n \n);\n\n\n\n\n\n\nTemplates\n\n\nPrefixed with \u201ct-\u201d in CSS, JavaScript, and HTML.\n\n\nCSS\n\n\n.\nt-careers\n \n{\n\n    \n_social\n \n.m-social-media\n \n{\n\n        \nfloat\n:\n \nright\n;\n\n    \n}\n\n    \n\u2026\n\n\n\n\n\n\nFolder Structure\n\n\nAtomic code is currently separated and named based on asset type. This is a mistake in my view, as I believe we should begin migrating to a modular folder structure based on the component.\n\n\nCurrent Structure\n\n\nHTML\n\n\ncfgov-refresh/cfgov/jinja2/v1/_includes/atoms/\ncfgov-refresh/cfgov/jinja2/v1/_includes/molecules/\ncfgov-refresh/cfgov/jinja2/v1/_includes/organisms/\n\n\n\n\n\nCSS\n\n\ncfgov-refresh/cfgov/unprocessed/css/atoms/\ncfgov-refresh/cfgov/unprocessed/css/molecules/\ncfgov-refresh/cfgov/unprocessed/css/organisms/\n\n\n\n\n\nJavaScript\n\n\ncfgov-refresh/cfgov/unprocessed/js/atoms/\ncfgov-refresh/cfgov/unprocessed/js/molecules/\ncfgov-refresh/cfgov/unprocessed/js/organisms/\n\n\n\n\n\nTest\n\n\ncfgov-refresh/test/unit_tests/atoms/\ncfgov-refresh/test/unit_tests/molecules/\ncfgov-refresh/test/unit_tests/organisms/\n\n\n\n\n\nProposed Folder Structure\n\n\ncfgov-refresh/cfgov/front-end/molecules/Expandable\n\nExpandable.html\nExpandable.css\nExpandable.js\nExpandable-unit-test.js\nREADME.MD\n\n\n\n\n\nJavaScript Architecture\n\n\nThere was considerable discussion on how we should create JS components. The components aren't constructed to be used on SPAs (Single Page Applications). They are built to be rendered on the sever and then enhanced via JavaScript on the client. The basic interface for the components is as follows:\n\n\nfunction\n \nAtomicComponent\n(\n \ndomElement\n \n)\n \n{\n\n    \n// Ensure the passed in Element is in the DOM.\n\n    \n// Query and store references to sub-elements.\n\n    \n// Instantiate child atomic components.\n\n    \n// Bind necessary events for referenced DOM elements.\n\n    \n// Perform other initialization related tasks.\n\n    \nthis\n.\ninit\n \n=\n \nfunction\n \ninit\n(){}\n\n\n    \n// General teardown function\n\n    \n// We don\nt remove the element from the DOM so\n\n    \n// we need to unbind the events.\n\n    \nthis\n.\ndestroy\n \n=\n \nfunction\n \ndestroy\n(){}\n\n\n}\n\n\n\n\n\n\nWe aren't testing for interface adherence but we probably should. We generally favor composition over inheritance. You can get more information by reading the following:\n\n\nArticles\n\n\nA Simple Challenge to Classical Inheritance Fans\n\n\nComposition over Inheritance (Youtube)\n\n\nCode and Related PRs\n\n\nView Unit Test\n\n\nExpandable example 1\n\n\nExpandable example 2\n\n\nComponent Build Pipeline\n\n\nGulp\n\n\nGulp is used as a task automation tool. A specific breakdown of each task is contained \nhere\n.\n\n\nWebpack\n\n\nWepback is used as a module bundler although it's capable of more. We create page, global, and atomic specific bundles. The configuration for the bundles is contained in webpack-config.js. An explanation for the usage of each bundle is contained in scripts.js.\n\n\nRoutes\n\n\nRoutes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page Media property. The happens via code contained in base.html.\n\n\nBase.html\n\n\nThis file serves as the base document for serving up assets and content. It's currently very complicated, obtrusive, and needs to be refactored.\n\n\nWagtail Page Media Property\n\n\nEach Atomic component has a media property which list the JavaScript files that should be rendered via base.html. When a page is requested via the browser, code contained in base.html will loop all Atomic components for the requested page and render the appropriate Atomic JavaScript bundles.\n\n\nQuestions and Concerns\n\n\n\n\nHow do we support Single Page Applications and be functional when JavaScript is disabled?\n\n\nHow do we ensure creation of performant atomic components?\n\n\nIs the codebase lacking uniformity?\n\n\nCSS bloat when multiple components are on the same page but from different Django apps.\n\n\nEnsuring simplicity over complexity.", 
            "title": "Atomic structure and design"
        }, 
        {
            "location": "/atomic-structure/#notes-on-atomic-design", 
            "text": "Check out  Don't Build Pages, Build Modules . It encompasses exactly what we are trying to achieve by building components using atomic design. It's important to note that our front-end atomic architecture is still evolving.  Our components are broken down into templates, organisms, molecules, and atoms. We opted not to use the page component, although it exists in atomic design. Our components are composed of HTML, CSS, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated js and/or css file. We compose our atomic components as follows:", 
            "title": "Notes on Atomic Design"
        }, 
        {
            "location": "/atomic-structure/#atoms", 
            "text": "Prefixed with \u201ca-\u201d in CSS, JavaScript, and HTML files.", 
            "title": "Atoms"
        }, 
        {
            "location": "/atomic-structure/#html", 
            "text": "div   class = a-overlay u-hidden / div", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css", 
            "text": ". a-overlay   { \n         //   Only   show   overlay   at   mobile/tablet   size. \n         .respond-to-max(   @bp-sm-max,   { \n             height :   100 % ; \n             width :   100 % ;", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#molecules", 
            "text": "Prefixed with \u201cm-\u201d in CSS, JavaScript, and HTML files.", 
            "title": "Molecules"
        }, 
        {
            "location": "/atomic-structure/#html_1", 
            "text": "div   class = m-notification m-notification__error m-notification__visible   data-js-hook = state_atomic_init \n       span   class = m-notification_icon cf-icon / span \n       div   class = m-notification_content   role = alert / div \n  / div", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css_1", 
            "text": ". m-notification   { \n     display :   none ; \n     position :   relative ; \n     padding :   @ m-notification-padding__px ; \n     padding-left :   40 px ;", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#javascript", 
            "text": "function   Notification (   element   )   { \n    // eslint-disable-line max-statements, inline-comments, max-len \n    var   BASE_CLASS   =   m-notification ; \n\n    // Constants for the state of this Notification. \n    var   SUCCESS   =   success ; \n    var   WARNING   =   warning ; \n    var   ERROR   =   error ; \n    // Constants for the Notification modifiers. \n    var   MODIFIER_VISIBLE   =   BASE_CLASS   +   __visible ; \n    var   _dom   =   atomicHelpers . checkDom (   element ,   BASE_CLASS   ); \n    var   _contentDom   =   _dom . querySelector (   .   +   BASE_CLASS   +   _content   );   The notification molecule can be instantiated with the following code:  _notification   =   new   Notification (   _dom   );  _notification . init ();", 
            "title": "JavaScript"
        }, 
        {
            "location": "/atomic-structure/#organisms", 
            "text": "Prefixed with \u201co-\u201d in CSS, JavaScript, and HTML.", 
            "title": "Organisms"
        }, 
        {
            "location": "/atomic-structure/#html_2", 
            "text": "div   data-qa-hook = expandable   class = o-expandable                                        o-expandable__borders                                        o-expandable__midtone                                        o-expandable__expanded \n                                data-js-hook = state_atomic_init \n     button   class = o-expandable_target   aria-pressed = true \n         div   class = o-expandable_header   JavaScript:    function   Expandable (   element   )   { \n   var   BASE_CLASS   =   o-expandable ; \n\n   // Bitwise flags for the state of this Expandable. \n   var   COLLAPSED   =   0 ; \n   var   COLLAPSING   =   1 ; \n   var   EXPANDING   =   2 ; \n   var   EXPANDED   =   3 ; \n\n   // The Expandable element will directly be the Expandable \n   // when used in an ExpandableGroup, otherwise it can be the parent container. \n   var   _dom   =   atomicHelpers . checkDom (   element ,   BASE_CLASS   ); \n   var   _target   =   _dom . querySelector (   .   +   BASE_CLASS   +   _target   ); \n   var   _content   =   _dom . querySelector (   .   +   BASE_CLASS   +   _content   );   The Expandable organism can be instantiated with the following code:  _expandable   =   new   Expandable (   _dom . querySelector (   .o-expandable   )   );  _expandable . init (   _expandable . EXPANDED   );   or  var   atomicHelpers   =   require (   ../../modules/util/atomic-helpers   );  var   Expandable   =   require (   ../../organisms/Expandable   );  atomicHelpers . instantiateAll (   .o-expandable ,   Expandable   );", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#templates", 
            "text": "Prefixed with \u201ct-\u201d in CSS, JavaScript, and HTML.", 
            "title": "Templates"
        }, 
        {
            "location": "/atomic-structure/#css_2", 
            "text": ". t-careers   { \n     _social   .m-social-media   { \n         float :   right ; \n     } \n     \u2026", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#folder-structure", 
            "text": "Atomic code is currently separated and named based on asset type. This is a mistake in my view, as I believe we should begin migrating to a modular folder structure based on the component.", 
            "title": "Folder Structure"
        }, 
        {
            "location": "/atomic-structure/#current-structure", 
            "text": "", 
            "title": "Current Structure"
        }, 
        {
            "location": "/atomic-structure/#html_3", 
            "text": "cfgov-refresh/cfgov/jinja2/v1/_includes/atoms/\ncfgov-refresh/cfgov/jinja2/v1/_includes/molecules/\ncfgov-refresh/cfgov/jinja2/v1/_includes/organisms/", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css_3", 
            "text": "cfgov-refresh/cfgov/unprocessed/css/atoms/\ncfgov-refresh/cfgov/unprocessed/css/molecules/\ncfgov-refresh/cfgov/unprocessed/css/organisms/", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#javascript_1", 
            "text": "cfgov-refresh/cfgov/unprocessed/js/atoms/\ncfgov-refresh/cfgov/unprocessed/js/molecules/\ncfgov-refresh/cfgov/unprocessed/js/organisms/", 
            "title": "JavaScript"
        }, 
        {
            "location": "/atomic-structure/#test", 
            "text": "cfgov-refresh/test/unit_tests/atoms/\ncfgov-refresh/test/unit_tests/molecules/\ncfgov-refresh/test/unit_tests/organisms/", 
            "title": "Test"
        }, 
        {
            "location": "/atomic-structure/#proposed-folder-structure", 
            "text": "cfgov-refresh/cfgov/front-end/molecules/Expandable\n\nExpandable.html\nExpandable.css\nExpandable.js\nExpandable-unit-test.js\nREADME.MD", 
            "title": "Proposed Folder Structure"
        }, 
        {
            "location": "/atomic-structure/#javascript-architecture", 
            "text": "There was considerable discussion on how we should create JS components. The components aren't constructed to be used on SPAs (Single Page Applications). They are built to be rendered on the sever and then enhanced via JavaScript on the client. The basic interface for the components is as follows:  function   AtomicComponent (   domElement   )   { \n     // Ensure the passed in Element is in the DOM. \n     // Query and store references to sub-elements. \n     // Instantiate child atomic components. \n     // Bind necessary events for referenced DOM elements. \n     // Perform other initialization related tasks. \n     this . init   =   function   init (){} \n\n     // General teardown function \n     // We don t remove the element from the DOM so \n     // we need to unbind the events. \n     this . destroy   =   function   destroy (){}  }   We aren't testing for interface adherence but we probably should. We generally favor composition over inheritance. You can get more information by reading the following:", 
            "title": "JavaScript Architecture"
        }, 
        {
            "location": "/atomic-structure/#articles", 
            "text": "A Simple Challenge to Classical Inheritance Fans  Composition over Inheritance (Youtube)", 
            "title": "Articles"
        }, 
        {
            "location": "/atomic-structure/#code-and-related-prs", 
            "text": "View Unit Test  Expandable example 1  Expandable example 2", 
            "title": "Code and Related PRs"
        }, 
        {
            "location": "/atomic-structure/#component-build-pipeline", 
            "text": "", 
            "title": "Component Build Pipeline"
        }, 
        {
            "location": "/atomic-structure/#gulp", 
            "text": "Gulp is used as a task automation tool. A specific breakdown of each task is contained  here .", 
            "title": "Gulp"
        }, 
        {
            "location": "/atomic-structure/#webpack", 
            "text": "Wepback is used as a module bundler although it's capable of more. We create page, global, and atomic specific bundles. The configuration for the bundles is contained in webpack-config.js. An explanation for the usage of each bundle is contained in scripts.js.", 
            "title": "Webpack"
        }, 
        {
            "location": "/atomic-structure/#routes", 
            "text": "Routes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page Media property. The happens via code contained in base.html.", 
            "title": "Routes"
        }, 
        {
            "location": "/atomic-structure/#basehtml", 
            "text": "This file serves as the base document for serving up assets and content. It's currently very complicated, obtrusive, and needs to be refactored.", 
            "title": "Base.html"
        }, 
        {
            "location": "/atomic-structure/#wagtail-page-media-property", 
            "text": "Each Atomic component has a media property which list the JavaScript files that should be rendered via base.html. When a page is requested via the browser, code contained in base.html will loop all Atomic components for the requested page and render the appropriate Atomic JavaScript bundles.", 
            "title": "Wagtail Page Media Property"
        }, 
        {
            "location": "/atomic-structure/#questions-and-concerns", 
            "text": "How do we support Single Page Applications and be functional when JavaScript is disabled?  How do we ensure creation of performant atomic components?  Is the codebase lacking uniformity?  CSS bloat when multiple components are on the same page but from different Django apps.  Ensuring simplicity over complexity.", 
            "title": "Questions and Concerns"
        }, 
        {
            "location": "/notes-on-atomic-design/", 
            "text": "Atomic Design in Capital Framework\n\n\nTLDR\n\n\n\n\nNot easy to update bits and pieces (will require a full versioned release)\n\n\nA few components that haven't been converted by the v1 team yet (possibly workshop with entire D\nD team):\n\n\nHero\n\n\nFeature Content Module\n\n\nForms (partially done)\n\n\n\n\n\n\nDifficult to test how changes affect every project (we should set up a playground)\n\n\nUnclear how atomic changes will affect components\n\n\nDocumentation is incomplete (working on that currently)\n\n\nUnclear how future changes will be updated in Wagtail (should platform own this?)\n\n\n\n\nDetails\n\n\nNow that v1 is out there, we've been getting a lot of interest about updating Capital Framework with the Atomic Design Principles to build better reusable and consistent design patterns. As we complete the documentation process, I've been collecting concerns, thoughts, and next steps we'll need to take to make that possible. Unfortunately, a large part of understanding Atomic Design happens when working with it yourself, but we can make that process easier by updating Capital Framework to utilize and document the patterns we've built.\n\n\nIt's not easy to update bits and pieces\n\n\nWhile it'd be nice to update Capital Framework one module at a time, much like we did during the Design Surge, it's super complicated to mix atomic and non-atomic components. You end up with either duplication of code, or an entanglement between the components. Instead, it makes more sense to start an atomic release branch and port components there.\n\n\nComponents would be ported as time allows, then released as whole as the next major version (aka v4). Older, unsupported projects could stay on v3, but newer projects would be updated to v4. This means some duplication of work going forward, but similar to jQuery and other projects, we could set a sunset date for v3 and only provide critical updates past that date.\n\n\nThere are some missing and unfinished components\n\n\nWe covered quite a lot in v1, but there are a few components that haven't been converted and some that require further work. At the same time, one of the lessons we learned during the process is that it's difficult to understand all of the principles without working with them. While the Platform team will be taking on much of this work, there's no reason to lay it all on their shoulders, and spreading the work around will enable those outside v1 and Platform to become more comfortable with designing and building atomically.\n\n\nSo far I've received a good response to hosting a half-day workshop for both designers and developers at the July regroup to go over the principles and begin converting the Hero and FCM. A group event would allow us to pass along our knowledge and lessons learned in person, while also giving the rest of the team a chance to work with real designs and code, making it a lot easier to either pick up unfinished work or produce new components or changes based on project work.\n\n\nIt's difficult to test how changes will affect other projects\n\n\nWhile working within v1 it was pretty easy to test if a change to a component had unintended consequences elsewhere because everything was contained within the project. Despite that, unintended changes did still happen here and there. Once the atomic components are distributed to other projects, the chances of unintended changes becomes a lot higher. While NPM installing your local version of Capital Framework does make it easier to test, standing up cfgov-refresh with all of it's sibling projects is still a burden on those not consistently working on it.\n\n\nA playground that lives within Capital Framework (but not the documentation site) would make testing for edge cases and unintended issues far easier. As changes are proposed, we would have immediate feedback from live examples of our code base as it exists in production projects. The playground doesn't need to be a full blown live editor like CodePen, static examples that are copied from production to this playground should be more than enough to pass some gut checks.\n\n\nIt's unclear how atomic changes will affect current component organization\n\n\nHow will atoms, molecules, and organisms be arranged within a component? How will these changes affect other components that then import those pieces? What happens if a molecule needs to become an organism, or vice versa? These questions will probably need to be fleshed out as we complete the transition work, but it's good to keep them in mind as we go along.\n\n\nIt's unclear how component changes will be propagated to Wagtail\n\n\nIf a component's foundation changes (markup, options, etc) how will those changes be passed into the CMS? Will it be the responsibility of the FEWD to make them or will the work be passed to a Platform team BEWD. How will that affect their workload and how should we pass that task along? What will be a reasonable timeline?\n\n\nFor example, if we pass a task to the Platform team mid-sprint, but it couldn't be tackled for one or more sprints, it will create a delay in the release cycle of CF possibly leading to stale code. We will need to be more conscious and do some long term planning in situations where updates come from projects outside of Platform.\n\n\nThe documentation is incomplete\n\n\nThe v1 designers have been working on thorough documentation of the components we've created, but the Capital Framework documentation will need to be updated as well. To ensure it happens (I am at fault for this as much as anyone), documentation updates should be strictly enforced when component PRs are opened. If we don't stay on top of it, it'll be the thing that gets pushed off indefinitely.", 
            "title": "Notes on adapting atomic design"
        }, 
        {
            "location": "/notes-on-atomic-design/#atomic-design-in-capital-framework", 
            "text": "", 
            "title": "Atomic Design in Capital Framework"
        }, 
        {
            "location": "/notes-on-atomic-design/#tldr", 
            "text": "Not easy to update bits and pieces (will require a full versioned release)  A few components that haven't been converted by the v1 team yet (possibly workshop with entire D D team):  Hero  Feature Content Module  Forms (partially done)    Difficult to test how changes affect every project (we should set up a playground)  Unclear how atomic changes will affect components  Documentation is incomplete (working on that currently)  Unclear how future changes will be updated in Wagtail (should platform own this?)", 
            "title": "TLDR"
        }, 
        {
            "location": "/notes-on-atomic-design/#details", 
            "text": "Now that v1 is out there, we've been getting a lot of interest about updating Capital Framework with the Atomic Design Principles to build better reusable and consistent design patterns. As we complete the documentation process, I've been collecting concerns, thoughts, and next steps we'll need to take to make that possible. Unfortunately, a large part of understanding Atomic Design happens when working with it yourself, but we can make that process easier by updating Capital Framework to utilize and document the patterns we've built.", 
            "title": "Details"
        }, 
        {
            "location": "/notes-on-atomic-design/#its-not-easy-to-update-bits-and-pieces", 
            "text": "While it'd be nice to update Capital Framework one module at a time, much like we did during the Design Surge, it's super complicated to mix atomic and non-atomic components. You end up with either duplication of code, or an entanglement between the components. Instead, it makes more sense to start an atomic release branch and port components there.  Components would be ported as time allows, then released as whole as the next major version (aka v4). Older, unsupported projects could stay on v3, but newer projects would be updated to v4. This means some duplication of work going forward, but similar to jQuery and other projects, we could set a sunset date for v3 and only provide critical updates past that date.", 
            "title": "It's not easy to update bits and pieces"
        }, 
        {
            "location": "/notes-on-atomic-design/#there-are-some-missing-and-unfinished-components", 
            "text": "We covered quite a lot in v1, but there are a few components that haven't been converted and some that require further work. At the same time, one of the lessons we learned during the process is that it's difficult to understand all of the principles without working with them. While the Platform team will be taking on much of this work, there's no reason to lay it all on their shoulders, and spreading the work around will enable those outside v1 and Platform to become more comfortable with designing and building atomically.  So far I've received a good response to hosting a half-day workshop for both designers and developers at the July regroup to go over the principles and begin converting the Hero and FCM. A group event would allow us to pass along our knowledge and lessons learned in person, while also giving the rest of the team a chance to work with real designs and code, making it a lot easier to either pick up unfinished work or produce new components or changes based on project work.", 
            "title": "There are some missing and unfinished components"
        }, 
        {
            "location": "/notes-on-atomic-design/#its-difficult-to-test-how-changes-will-affect-other-projects", 
            "text": "While working within v1 it was pretty easy to test if a change to a component had unintended consequences elsewhere because everything was contained within the project. Despite that, unintended changes did still happen here and there. Once the atomic components are distributed to other projects, the chances of unintended changes becomes a lot higher. While NPM installing your local version of Capital Framework does make it easier to test, standing up cfgov-refresh with all of it's sibling projects is still a burden on those not consistently working on it.  A playground that lives within Capital Framework (but not the documentation site) would make testing for edge cases and unintended issues far easier. As changes are proposed, we would have immediate feedback from live examples of our code base as it exists in production projects. The playground doesn't need to be a full blown live editor like CodePen, static examples that are copied from production to this playground should be more than enough to pass some gut checks.", 
            "title": "It's difficult to test how changes will affect other projects"
        }, 
        {
            "location": "/notes-on-atomic-design/#its-unclear-how-atomic-changes-will-affect-current-component-organization", 
            "text": "How will atoms, molecules, and organisms be arranged within a component? How will these changes affect other components that then import those pieces? What happens if a molecule needs to become an organism, or vice versa? These questions will probably need to be fleshed out as we complete the transition work, but it's good to keep them in mind as we go along.", 
            "title": "It's unclear how atomic changes will affect current component organization"
        }, 
        {
            "location": "/notes-on-atomic-design/#its-unclear-how-component-changes-will-be-propagated-to-wagtail", 
            "text": "If a component's foundation changes (markup, options, etc) how will those changes be passed into the CMS? Will it be the responsibility of the FEWD to make them or will the work be passed to a Platform team BEWD. How will that affect their workload and how should we pass that task along? What will be a reasonable timeline?  For example, if we pass a task to the Platform team mid-sprint, but it couldn't be tackled for one or more sprints, it will create a delay in the release cycle of CF possibly leading to stale code. We will need to be more conscious and do some long term planning in situations where updates come from projects outside of Platform.", 
            "title": "It's unclear how component changes will be propagated to Wagtail"
        }, 
        {
            "location": "/notes-on-atomic-design/#the-documentation-is-incomplete", 
            "text": "The v1 designers have been working on thorough documentation of the components we've created, but the Capital Framework documentation will need to be updated as well. To ensure it happens (I am at fault for this as much as anyone), documentation updates should be strictly enforced when component PRs are opened. If we don't stay on top of it, it'll be the thing that gets pushed off indefinitely.", 
            "title": "The documentation is incomplete"
        }, 
        {
            "location": "/notes-on-frameworks/", 
            "text": "Notes on Frameworks\n\n\nWe opted not to use any frameworks; the application isn't a SPA (single page application) and we didn't think it's level of complexity warranted using one. This might be a mistake and it's worth revisiting this decision, now that we have migrated to Atomic design.\n\n\nThe pros / cons of that decision are as follows:\n\n\nPros\n\n\n\n\nSmaller builds and minimal markup ( JS, CSS, HTML). Many frameworks are bloated and applications only use a small portion of their functionality / features.\n\n\nLearning curve is drastically reduced by not requiring people to learn a new framework.\n\n\nEasier for developers to architect / debug the application by requiring a better understanding of DOM, JS modules, and CSS.\n\n\nSome frameworks have ventured off the pure JS path, making them inadequate tools for new developers learning JS (React with Redux, Angular 2 via Typescript).\n\n\n\n\nCons\n\n\n\n\nDevelopment time is increased by not relying upon existing frameworks and components.\n\n\nNot having a standard way to build components can lead to confusion and non-uniform code.\n\n\nForces developers to really understand DOM manipulation and it's impact on performance (reflows/repaints). React, Angular, and Amp all manage DOM interactions for you through virtual dom and buffering reads / writes. There are very, very few devs that are good at doing this without a framework or library.\n\n\n\n\nShould projects use frameworks?\n\n\nThe decision to use a framework should be made on a project-by-project basis. It should be driven by the team make-up, project goals, and scope. The only framework we would recommend avoiding at this time is Angular. The Angular project is in a state of flux and it would be better to wait until Angular 2 is a bit more mature.", 
            "title": "Notes on frameworks"
        }, 
        {
            "location": "/notes-on-frameworks/#notes-on-frameworks", 
            "text": "We opted not to use any frameworks; the application isn't a SPA (single page application) and we didn't think it's level of complexity warranted using one. This might be a mistake and it's worth revisiting this decision, now that we have migrated to Atomic design.  The pros / cons of that decision are as follows:", 
            "title": "Notes on Frameworks"
        }, 
        {
            "location": "/notes-on-frameworks/#pros", 
            "text": "Smaller builds and minimal markup ( JS, CSS, HTML). Many frameworks are bloated and applications only use a small portion of their functionality / features.  Learning curve is drastically reduced by not requiring people to learn a new framework.  Easier for developers to architect / debug the application by requiring a better understanding of DOM, JS modules, and CSS.  Some frameworks have ventured off the pure JS path, making them inadequate tools for new developers learning JS (React with Redux, Angular 2 via Typescript).", 
            "title": "Pros"
        }, 
        {
            "location": "/notes-on-frameworks/#cons", 
            "text": "Development time is increased by not relying upon existing frameworks and components.  Not having a standard way to build components can lead to confusion and non-uniform code.  Forces developers to really understand DOM manipulation and it's impact on performance (reflows/repaints). React, Angular, and Amp all manage DOM interactions for you through virtual dom and buffering reads / writes. There are very, very few devs that are good at doing this without a framework or library.", 
            "title": "Cons"
        }, 
        {
            "location": "/notes-on-frameworks/#should-projects-use-frameworks", 
            "text": "The decision to use a framework should be made on a project-by-project basis. It should be driven by the team make-up, project goals, and scope. The only framework we would recommend avoiding at this time is Angular. The Angular project is in a state of flux and it would be better to wait until Angular 2 is a bit more mature.", 
            "title": "Should projects use frameworks?"
        }, 
        {
            "location": "/testing/", 
            "text": "Browser tests\n\n\nQuick start:\n\n\nTo run browser tests, open a new Terminal window or tab and change to the project directory,\nthen tell gulp to start the tests:\n\n\ngulp build\ngulp test:acceptance \n(\n tox -e acceptance can be run as well \n)\n\n\n\n\n\n\nThere are several options you can pass to run a particular suite of tests,\nto run a particular list of features,\nand/or to run it in \"fast\" mode:\n\n\ngulp test:acceptance --suite\n=\nwagtail-admin \n(\n runs just the wagtail-admin suite \n)\n\ngulp test:acceptance --specs\n=\nmulti-select.feature \n(\n runs just the multi-select feature \n)\n\ngulp test:acceptance --tags\n=\n@mobile \n(\n runs all scenarios tagged with @mobile \n)\n\ngulp test:acceptance --fast \n(\n runs the tests without recreating the virtual environment \n)\n\n\n\n\n\n\nThe same options can be used with tox (--omitted):\n\n\ntox -e acceptance \nsuite\n=\nwagtail-admin\ntox -e acceptance \nspecs\n=\nmulti-select.feature\ntox -e acceptance \ntags\n=\n@mobile\ntox -e acceptance-fast\n\n\n\n\n\nThese tests will run on their own server; you do not need to be running your development server.\n\n\nCucumber - tool for running automated tests written in plain language\n\n\nBelow are some suggested standards for Cucumber Feature files:\n\n\nTable copied from \nhttps://saucelabs.com/blog/write-great-cucumber-tests\n by Greg Sypolt, with moderate modifications\n\n\n\n   \n\n      \n\n         \nFeature Files\n\n         \nEvery *.feature file consists in a single feature, focused on the business value.\n\n      \n\n      \n\n      \nGherkin\n\n         \n\n            \nFeature:Title (one line describing the story)\nNarrative Description: As a [role], I want [feature], so that I [benefit]\n\nScenario: Title (acceptance criteria of user story)\n  Given [context]\n  And [some more context]...\n  When [event]\n  Then [outcome]\n  And [another outcome]...\n\nScenario:...\n\n\n\n\n      \n\n      \n\n         \nGiven, When, and Then Statements\n\n         \n\n           There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose. \n\n\n\n\nGiven\n is the pre-condition to put the system into a known state before the user starts interacting with the application\n\n\nWhen\n describes the key action the user performs\n\n\nThen\n is observing the expected outcome\n\n\n\n\nJust remember the \n\u2018then\u2019\n step is an acceptance criteria of the story.\n   \n\n      \n\n      \n\n         \nBackground\n\n         \nThe background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario.\n\n\nBackground:\n  Given I am logged into Wagtail as an admin\n  And I create a Wagtail Sublanding Page\n  And I open the content menu\n\n        \n\n      \n\n      \n\n         \nScenarios\n\n         \nKeep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios.  Scenarios should be between 3 to 6 statements, if possible.\n\n      \n\n      \n\n         \nScenario Outline\n\n         \nIf you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing.\n\n      \n\n      \n\n         \nDeclarative Vs Imperative Scenarios\n\n         \n\n            The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application.  The imperative style is more verbose but better describes the expected behavior.  Either style is acceptable.\n\n\nExample: Declarative\n\n\n\nScenario:User logs in\n  Given I am on the homepage\n  When I log in\n  Then I should see a login notification\n\n\n\nExample: Imperative\n\n\n\nScenario: User logs in\n  Given I am on the homepage\n  When I click on the \"Login\" button\n  And I fill in the \"Email\" field with \"\n\"\n  And I fill in the \"Password\" field with \"secret\"\n  And I click on \"Submit\"\n  Then I should see \"Welcome to the app, John Doe\"\n\n\n         \n\n      \n\n   \n\n\n\n\nSauce Connect - send tests to the cloud\n\n\n\n\nDanger\n\n\nThe instruction for automatically\nrunning Sauce Connect from gulp are not working.\nSee \nhttps://github.com/cfpb/cfgov-refresh/issues/2324\n\n\n\n\nSauce Labs can be used to run tests remotely in the cloud.\n\n\n\n\n\n\nLog into \nhttp://saucelabs.com/account\n.\n\n\n\n\n\n\nDownload Sauce Connect\n\n\n\n\n\n\nOpen a new Terminal window or tab and navigate to the downloaded SauceConnect folder.\n    If you place the folder in your Application's folder this might look like:\n\n\ncd /Users/\nYOUR MAC OSX USERNAME\n/Applications/SauceConnect\n\n\n\n\n\n\n\n\n\nCopy step 3 from the the SauceLabs\n   \nBasic Setup instructions\n\n   and run that in your Terminal window.\n   Once you see \nSauce Connect is up\n in the Terminal,\n   that means the tunnel has successfully been established\n\n\n\n\nThe Terminal command should already have your Sauce username and access key filled in.\n  If it doesn't, make sure you're logged in.\n\n\n\n\n\n\n\n\nUpdate and uncomment the \nSAUCE_USERNAME\n, \nSAUCE_ACCESS_KEY\n,\n   and \nSAUCE_SELENIUM_URL\n values in your \n.env\n file.\n   The access key can be found in lower-left on the Sauce Labs\n   \naccount profile page\n.\n\n\n\n\n\n\nReload the settings with \ncd .. \n cd cfgov-refresh\n. Type \ny\n if prompted.\n\n\n\n\n\n\nRun the tests with \ngulp test:acceptance\n.\n\n\n\n\nNote\n\n\nIf you want to temporarily disable testing on Sauce Labs,\nrun the command as: \ngulp test:acceptance --sauce=false\n.\n\n\n\n\n\n\n\n\nMonitor progress of the tests\n   on the \nSauce Labs dashboard\n Automated Tests tab.\n\n\n\n\n\n\n\n\nNote\n\n\nIf you get the error \nError: ENOTFOUND getaddrinfo ENOTFOUND\n\nwhile running a test, it likely means that Sauce Connect is not running.\nSee step 4 above.\n\n\n\n\nManual test configuration\n\n\nA number of command-line arguments can be set to test particular configurations:\n\n\n\n\n--suite\n: Choose a particular suite or suites to run.\n   For example, \ngulp test:acceptance --suite=content\n or \ngulp test:acceptance --suite=content,functional\n.\n\n\n--specs\n: Choose a particular spec or specs to run.\n   For example, \ngulp test:acceptance --specs=contact-us.js\n, \ngulp test:acceptance --specs=contact-us.js,about-us.js\n, or \ngulp test:acceptance --specs=foo*.js\n. If \n--suite\n is specified, this argument will be ignored. If neither \n--suite\n nor \n--specs\n are specified, all specs will be run.\n\n\n--windowSize\n: Set the window size in pixels in \nw,h\n format.\n   For example, \ngulp test:acceptance --windowSize=900,400\n.\n\n\n--browserName\n: Set the browser to run.\n   For example, \ngulp test:acceptance --browserName=firefox\n.\n\n\n--version\n: Set the browser version to run.\n   For example, \ngulp test:acceptance --version='44.0'\n.\n\n\n--platform\n: Set the OS platform to run.\n   For example, \ngulp test:acceptance --platform='osx 10.10'\n.\n\n\n--sauce\n: Whether to run on Sauce Labs or not.\n   For example, \ngulp test:acceptance --sauce=false\n.\n\n\n\n\nTests\n\n\nTests are organized into suites under the \ntest/browser_tests/cucumber/features\n directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a \n.feature\n spec in one of these suites, and then adding corresponding step definitions, found in \ntest/browser_tests/cucumber/step_definitions\n.\n\n\nFurther reading\n\n\n\n\nCucumber features\n\n\nProtractor\n\n\nSelect elements on a page\n\n\nWriting Jasmin expectations\n.\n\n\nUnderstanding Page Objects\n\n\n\n\nPerformance testing\n\n\nTo audit if the site complies with performance best practices and guidelines,\nrun \ngulp test:perf\n.\n\n\nThe audit will run against\n\nGoogle's PageSpeed Insights\n.\n\n\nDjango and Python unit tests\n\n\nTo run the the full suite of Python 2.7 unit tests using Tox, cd to the project\nroot, make sure the \nTOXENV\n variable is set in your \n.env\n file and then run\n\n\ntox\n\n\n\n\n\nIf you haven't changed any installed packages and you don't need to test all migrations, you can run a much faster Python code test using:\n\ntox -e fast\n\n\n\n\n\nTo see Python code coverage information, run\n\n./show_coverage.sh\n\n\n\n\n\nAccessibility Testing\n\n\nRun the acceptance tests with an \n--a11y\n flag (i.e. \ngulp test:acceptance --a11y\n)\nto check every webpage for WCAG and Section 508 compliancy using Protractor's\n\naccessibility plugin\n.\n\n\nIf you'd like to audit a specific page, use \ngulp test:a11y\n:\n\n\n\n\nEnable the environment variable \nACHECKER_ID\n in your \n.env\n file.\n     Get a free \nAChecker API ID\n for the value.\n\n\nReload your \n.env\n with \nsource ./.env\n while in the project root directory.\n\n\nRun \ngulp test:a11y\n to run an audit on the homepage.\n\n\nTo test a page aside from the homepage, add the \n--u=\npath_to_test\n flag.\n     For example, \ngulp test:a11y --u=contact-us\n\n     or \ngulp test:a11y --u=the-bureau/bureau-structure/\n.\n\n\n\n\nSource code linting\n\n\nThe default test task includes linting of the JavaScript source, build,\nand test files.\nUse the \ngulp lint\n command from the command-line to run the ESLint linter,\nwhich checks the JavaScript against the rules configured in \n.eslintrc\n.\n\nSee the ESLint docs\n\nfor detailed rule descriptions.\n\n\nThere are a number of options to the command:\n\n\n\n\ngulp lint:build\n: Lint only the gulp build scripts.\n\n\ngulp lint:test\n: Lint only the test scripts.\n\n\ngulp lint:scripts\n: Lint only the project source scripts.\n\n\n--fix\n: Add this flag (like \ngulp lint --fix\n or \ngulp lint:build --fix\n)\n   to auto-fix some errors, where ESLint has support to do so.", 
            "title": "Testing"
        }, 
        {
            "location": "/testing/#browser-tests", 
            "text": "", 
            "title": "Browser tests"
        }, 
        {
            "location": "/testing/#quick-start", 
            "text": "To run browser tests, open a new Terminal window or tab and change to the project directory,\nthen tell gulp to start the tests:  gulp build\ngulp test:acceptance  (  tox -e acceptance can be run as well  )   There are several options you can pass to run a particular suite of tests,\nto run a particular list of features,\nand/or to run it in \"fast\" mode:  gulp test:acceptance --suite = wagtail-admin  (  runs just the wagtail-admin suite  ) \ngulp test:acceptance --specs = multi-select.feature  (  runs just the multi-select feature  ) \ngulp test:acceptance --tags = @mobile  (  runs all scenarios tagged with @mobile  ) \ngulp test:acceptance --fast  (  runs the tests without recreating the virtual environment  )   The same options can be used with tox (--omitted):  tox -e acceptance  suite = wagtail-admin\ntox -e acceptance  specs = multi-select.feature\ntox -e acceptance  tags = @mobile\ntox -e acceptance-fast  These tests will run on their own server; you do not need to be running your development server.", 
            "title": "Quick start:"
        }, 
        {
            "location": "/testing/#cucumber-tool-for-running-automated-tests-written-in-plain-language", 
            "text": "Below are some suggested standards for Cucumber Feature files:  Table copied from  https://saucelabs.com/blog/write-great-cucumber-tests  by Greg Sypolt, with moderate modifications  \n    \n       \n          Feature Files \n          Every *.feature file consists in a single feature, focused on the business value. \n       \n       \n       Gherkin \n          \n             Feature:Title (one line describing the story)\nNarrative Description: As a [role], I want [feature], so that I [benefit] \nScenario: Title (acceptance criteria of user story)\n  Given [context]\n  And [some more context]...\n  When [event]\n  Then [outcome]\n  And [another outcome]... \nScenario:...  \n       \n       \n          Given, When, and Then Statements \n          \n           There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose.    Given  is the pre-condition to put the system into a known state before the user starts interacting with the application  When  describes the key action the user performs  Then  is observing the expected outcome   Just remember the  \u2018then\u2019  step is an acceptance criteria of the story.\n    \n       \n       \n          Background \n          The background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario. \nBackground:\n  Given I am logged into Wagtail as an admin\n  And I create a Wagtail Sublanding Page\n  And I open the content menu \n         \n       \n       \n          Scenarios \n          Keep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios.  Scenarios should be between 3 to 6 statements, if possible. \n       \n       \n          Scenario Outline \n          If you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing. \n       \n       \n          Declarative Vs Imperative Scenarios \n          \n            The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application.  The imperative style is more verbose but better describes the expected behavior.  Either style is acceptable.  Example: Declarative  \nScenario:User logs in\n  Given I am on the homepage\n  When I log in\n  Then I should see a login notification  Example: Imperative  \nScenario: User logs in\n  Given I am on the homepage\n  When I click on the \"Login\" button\n  And I fill in the \"Email\" field with \" \"\n  And I fill in the \"Password\" field with \"secret\"\n  And I click on \"Submit\"\n  Then I should see \"Welcome to the app, John Doe\"", 
            "title": "Cucumber - tool for running automated tests written in plain language"
        }, 
        {
            "location": "/testing/#sauce-connect-send-tests-to-the-cloud", 
            "text": "Danger  The instruction for automatically\nrunning Sauce Connect from gulp are not working.\nSee  https://github.com/cfpb/cfgov-refresh/issues/2324   Sauce Labs can be used to run tests remotely in the cloud.    Log into  http://saucelabs.com/account .    Download Sauce Connect    Open a new Terminal window or tab and navigate to the downloaded SauceConnect folder.\n    If you place the folder in your Application's folder this might look like:  cd /Users/ YOUR MAC OSX USERNAME /Applications/SauceConnect    Copy step 3 from the the SauceLabs\n    Basic Setup instructions \n   and run that in your Terminal window.\n   Once you see  Sauce Connect is up  in the Terminal,\n   that means the tunnel has successfully been established   The Terminal command should already have your Sauce username and access key filled in.\n  If it doesn't, make sure you're logged in.     Update and uncomment the  SAUCE_USERNAME ,  SAUCE_ACCESS_KEY ,\n   and  SAUCE_SELENIUM_URL  values in your  .env  file.\n   The access key can be found in lower-left on the Sauce Labs\n    account profile page .    Reload the settings with  cd ..   cd cfgov-refresh . Type  y  if prompted.    Run the tests with  gulp test:acceptance .   Note  If you want to temporarily disable testing on Sauce Labs,\nrun the command as:  gulp test:acceptance --sauce=false .     Monitor progress of the tests\n   on the  Sauce Labs dashboard  Automated Tests tab.     Note  If you get the error  Error: ENOTFOUND getaddrinfo ENOTFOUND \nwhile running a test, it likely means that Sauce Connect is not running.\nSee step 4 above.", 
            "title": "Sauce Connect - send tests to the cloud"
        }, 
        {
            "location": "/testing/#manual-test-configuration", 
            "text": "A number of command-line arguments can be set to test particular configurations:   --suite : Choose a particular suite or suites to run.\n   For example,  gulp test:acceptance --suite=content  or  gulp test:acceptance --suite=content,functional .  --specs : Choose a particular spec or specs to run.\n   For example,  gulp test:acceptance --specs=contact-us.js ,  gulp test:acceptance --specs=contact-us.js,about-us.js , or  gulp test:acceptance --specs=foo*.js . If  --suite  is specified, this argument will be ignored. If neither  --suite  nor  --specs  are specified, all specs will be run.  --windowSize : Set the window size in pixels in  w,h  format.\n   For example,  gulp test:acceptance --windowSize=900,400 .  --browserName : Set the browser to run.\n   For example,  gulp test:acceptance --browserName=firefox .  --version : Set the browser version to run.\n   For example,  gulp test:acceptance --version='44.0' .  --platform : Set the OS platform to run.\n   For example,  gulp test:acceptance --platform='osx 10.10' .  --sauce : Whether to run on Sauce Labs or not.\n   For example,  gulp test:acceptance --sauce=false .", 
            "title": "Manual test configuration"
        }, 
        {
            "location": "/testing/#tests", 
            "text": "Tests are organized into suites under the  test/browser_tests/cucumber/features  directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a  .feature  spec in one of these suites, and then adding corresponding step definitions, found in  test/browser_tests/cucumber/step_definitions .", 
            "title": "Tests"
        }, 
        {
            "location": "/testing/#further-reading", 
            "text": "Cucumber features  Protractor  Select elements on a page  Writing Jasmin expectations .  Understanding Page Objects", 
            "title": "Further reading"
        }, 
        {
            "location": "/testing/#performance-testing", 
            "text": "To audit if the site complies with performance best practices and guidelines,\nrun  gulp test:perf .  The audit will run against Google's PageSpeed Insights .", 
            "title": "Performance testing"
        }, 
        {
            "location": "/testing/#django-and-python-unit-tests", 
            "text": "To run the the full suite of Python 2.7 unit tests using Tox, cd to the project\nroot, make sure the  TOXENV  variable is set in your  .env  file and then run  tox  If you haven't changed any installed packages and you don't need to test all migrations, you can run a much faster Python code test using: tox -e fast   To see Python code coverage information, run ./show_coverage.sh", 
            "title": "Django and Python unit tests"
        }, 
        {
            "location": "/testing/#accessibility-testing", 
            "text": "Run the acceptance tests with an  --a11y  flag (i.e.  gulp test:acceptance --a11y )\nto check every webpage for WCAG and Section 508 compliancy using Protractor's accessibility plugin .  If you'd like to audit a specific page, use  gulp test:a11y :   Enable the environment variable  ACHECKER_ID  in your  .env  file.\n     Get a free  AChecker API ID  for the value.  Reload your  .env  with  source ./.env  while in the project root directory.  Run  gulp test:a11y  to run an audit on the homepage.  To test a page aside from the homepage, add the  --u= path_to_test  flag.\n     For example,  gulp test:a11y --u=contact-us \n     or  gulp test:a11y --u=the-bureau/bureau-structure/ .", 
            "title": "Accessibility Testing"
        }, 
        {
            "location": "/testing/#source-code-linting", 
            "text": "The default test task includes linting of the JavaScript source, build,\nand test files.\nUse the  gulp lint  command from the command-line to run the ESLint linter,\nwhich checks the JavaScript against the rules configured in  .eslintrc . See the ESLint docs \nfor detailed rule descriptions.  There are a number of options to the command:   gulp lint:build : Lint only the gulp build scripts.  gulp lint:test : Lint only the test scripts.  gulp lint:scripts : Lint only the project source scripts.  --fix : Add this flag (like  gulp lint --fix  or  gulp lint:build --fix )\n   to auto-fix some errors, where ESLint has support to do so.", 
            "title": "Source code linting"
        }, 
        {
            "location": "/development-tips/", 
            "text": "Development tips\n\n\nTIP: Updating npm shrinkwrapped dependencies\n\n\nFor \nsecurity reasons\n,\nthe dependencies in \npackage.json\n are pinned to a version\nin \nnpm-shrinkwrap.json\n.\nThis means updating a project dependency requires updating both files.\nThe easiest way to do this is the following steps:\n\n\n\n\nUpdate the version of the dependency in \npackage.json\n.\n\n\nDelete the \nnode_modules\n directory.\n\n\nDelete the \nnpm-shrinkwrap.json\n file.\n\n\nRun \n./frontend.sh shrinkwrap\n.\n\n\n\n\nCongrats! The dependency has been updated.\n\n\n\n\nNote\n\n\nShrinkwrapping will ignore development dependencies,\nso if one of those was updated you will need to run \nnpm install\n\nafter shrinkwrapping to bring your development environment up to date.\n\n\n\n\nTIP: Loading sibling projects\n\n\nSome projects fit within the cfgov-refresh architecture,\nbut are not fully incorporated into the project.\nThese are known as \"non-v1 Django apps.\"\nIn order to visit areas of the site locally where those projects are used,\nthe sibling projects need to be installed\nand then indexed within this cfgov-refresh project.\n\n\nThe non-v1 apps are the following:\n\n\n\n\nOwning a Home\n.\n\n\nfin-ed-resources (ghe/CFGOV/fin-ed-resources) - for the Education Resources section.\n\n\nknow-before-you-owe (ghe/CFGOV/know-before-you-owe) - for the Consumer Tools \n Know before you owe section.\n\n\n\n\nAfter installing these projects as sibling directories to the \ncfgov-refresh\n repository,\n\n\nOption 1: Sheer Index and Elasticsearch (e.g. owning-a-home)\n\n\n\n\nbuild the third-party projects per their directions,\n\n\nstop the web server and return to \ncfgov-refresh\n\n\nand run \ncfgov/manage.py sheer_index -r\n to load the projects' data into ElasticSearch.\n\n\n\n\nOption 2: Direct dependencies\n\n\n\n\nBuild the third-party projects per their directions\n\n\nStop the web server and return to \ncfgov-refresh\n\n\nRun \npip install -e ../\nsibling\n to load the projects' dependencies\n\n\n\n\n\n\nNote\n\n\nDo not install the projects directly into the \ncfgov-refresh\n directory.\nClone and install the projects as siblings to \ncfgov-refresh\n,\nso that they share the same parent directory (\n~/Projects\n or similar).\n\n\n\n\nTIP: Loading data into Django models\n\n\nThe Django management command \nimport-data\n will import data from the specified\nsource into the specified model.\n\n\nusage: manage.py import-data [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n        [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--parent PARENT]\n        [--snippet] -u USERNAME -p PASSWORD [--app APP] [--overwrite]\n        data_type wagtail_type\n\n\n\n- \ndata_type\n is the WordPress post type defined in the \nprocessors.py\n file.\n- \nwagtail_type\n is the Django model name where the data is going to go.\n- \n-u\n and \n-p\n are credentials to an admin account.\n\n\nRequired option:\n\n\n\n\n--parent\n is the slug of the parent page that the pages will exist\n  under.\n\n\n--snippet\n is a flag that's used to signify that the importing data will be\n  inserted into a Django model, registered as a \nWagtail snippet\n.\n  One of these options must be set for the command to run.\n\n\n\n\nOther options:\n\n\n\n\n--app\n is the name of the app the Django models from \nwagtail_type\n exist in.\n  It defaults to our app, \nv1\n.\n\n\n--overwrite\n overwrites existing pages in Wagtail based on comparing slugs.\nBe careful when using this as it will overwrite the data in Wagtail with data\nfrom the source. Default is \nFalse\n.\n\n\n--verbosity\n is set to 1 by default. Set it to 2 or higher and expect the\nname of the slugs to appear where appropriate.\n\n\n\n\nFor now, in order for this command to import the data, one of the things it\nneeds is a file for \"sheer logic\" to use to retrieve the data. For us, the\nprocessors are already done from our last backend. This part of the command\nwill change as we move away from our dependency on \"sheer logic.\" This is set\nby putting the file in a \nprocessors\n module in the top level of the project\nand adding it to the setting \nSHEER_PROCESSORS\n.\n\n\nThe command needs a \nprocessors\n module in the app that's passed to it, as well\nas a file with the same name as the Django model specified that defines a class\nnamed \nDataConverter\n that subclasses either \n_helpers.PageDataConverter\n or\n\n_helpers.SnippetDataConverter\n and implements their method(s) explained below:\n\n\nPageDataConverter\n:\n\n\n\n\nconvert(self, imported_data)\n:\n   For converting pages or snippets, the processor file must implement the\n   \nconvert()\n function with one argument. That argument represents the\n   imported data dictionary. That function must take the dictionary and map it\n   to a new one that uses the keys that Wagtail's \ncreate()\n and \nedit()\n\n   admin/snippet view functions expect in the \nrequest.POST\n dictionary to\n   actually migrate the data over, and then returns that dictionary where it\n   will be assigned to \nrequest.POST\n.\n\n\n\n\nSnippetDataConverter(PageDataConverter)\n:\n\n\n\n\nget_existing_snippet()\n\n   This also accepts the imported data dictionary. It's used to find an\n   existing snippet given the imported data, returning it if found or \nNone\n if\n   not.\n\n\n\n\nTIP: Working with the templates\n\n\nFront-End Template/Asset Locations\n\n\nTemplates\n that are served by the Django server: \ncfgov\\jinja2\\v1\n\n\nStatic assets\n prior to processing (minifying etc.): \ncfgov\\unprocessed\n.\n\n\n\n\nNote\n\n\nAfter running \ngulp build\n the site's assets are copied over to \ncfgov\\static_built\n,\nready to be served by Django.\n\n\n\n\nSimple static template setup\n\n\nBy default, Django will render pages with accordance to the URL pattern defined\nfor it. For example, going to \nhttp://localhost:8000/the-bureau/index.html\n\n(or \nhttp://localhost:8000/the-bureau/\n) renders \n/the-bureau/index.html\n from\nthe \ncfgov\n app folder's \njinja2/v1\n templates folder as processed\nby the \nJinja2\n templating engine.\n\n\nTIP: Outputting indexed content in a Sheer template\n\n\nMost of our content is indexed from the API output of our WordPress back-end.\nThis happens when the \npython cfgov/manage.py sheer_index -r\n command is run.\n\n\nThere are two ways in which we use indexed content:\nrepeating items (e.g., blog posts and press releases),\nand single pages (e.g., the Future Requests page in Doing Business with Us).\nWhat follows is a deeper dive into both of these content types.\n\n\nSingle content\n\n\nTo access a single piece of content,\nthe easiest thing to do is use the \nget_document()\n function.\n\n\nUsing the example given earlier of the Future Requests page,\nhere's how it's done:\n\n\n{%\n \nset\n \npage\n \n=\n \nget_document\n(\npages\n,\n \n63169\n)\n \n%}\n\n\n{{\n \npage.content\n \n|\n \nsafe\n \n}}\n\n\n\n\n\n\nThe \nget_document\n method can be used to retrieve a single item of any post type\nfor display within a template.\nIn the below example we get an instance of the non-hierarchical\n\ncontact\n post type using its slug (\nwhistleblowers\n):\n\n\n{%\n \nset\n \nwhistleblowers\n \n=\n \nget_document\n(\ncontact\n,\n \nwhistleblowers\n)\n \n%}\n\n\n\n\n\n\nIn practice, many of our templates are a Frankenstein-type mixture\nof hand-coded static content and calls to indexed content,\nas we continually try to strike the right balance of what content\nis appropriate to be edited by non-developers in Wagtail,\nand what is just too fragile to do any other way than by hand.\n\n\nTIP: Filtering results with queries\n\n\nSometimes you'll want to create queries in your templates to filter the data.\n\n\nThe two main ways of injecting filters into your data are in the URL's query\nstring and within the template code itself.\n\n\nWe have a handy function \nsearch()\n that:\n\n\n\n\nPulls in filters from the URL query string.\n\n\nAllows you to add additional filters by passing them in as arguments to the function.\n\n\n\n\nURL query string filters\n\n\nURL query string filters can be further broken down into two types:\n\n\n\n\nTerm - Used when you want to filter by whether a field matches a term.\nNote that in order to use this type of filter,\nthe field you are matching it against must have\n\n\"index\": \"not_analyzed\"\n set in the mapping.\n\n\nRange - Used for when you want to filter something by a range (e.g. dates or numbers)\n\n\n\n\nAn example of Term is:\n\n\n?filter_category=Op-Ed\n\n\nfilter_[field]=[value]\n\n\nAn example of Range is:\n\n\n?filter_range_date_gte=2014-01\n\n\nfilter_range_[field]_[operator]=[value]\n\n\nURL query string filters are convenient for many of the filtered queries you'll need to run,\nbut often there are cases where you'll need more flexibility.\n\n\nMore complex filters\n\n\nBy default, \nsearch()\n uses the default query parameters\ndefined in the \n_queries/object-name.json\n file,\nthen mixes them in with any additional arguments\nfrom the URL query string in addition to what is passed into the function itself.\n\n\nWhen using \nsearch()\n, you can also pass in filters with the same \nfilter_\n syntax as above.\n\n\nFor example:\n\n\nsearch(filter_category='Op-Ed')\n\n\nMultiple term filters on the same field will be combined in an OR clause, while\nterm filters of different fields will be combined in an AND clause.\n\n\nFor example:\n\n\nsearch(filter_tag='Students', filter_tag='Finance', filter_author='Batman')\n\n\nThis will return documents that have the tag Students OR Finance, AND have an author of Batman.\n\n\nIf you need more control over your filter than that,\nenter it manually in the \ncfgov/jinja2/v1/_queries/[filtername].json\n file.\n\n\nTIP: Debugging site performance\n\n\nWhen running locally it is possible to enable the\n\nDjango Debug Toolbar\n\nby defining the \nENABLE_DEBUG_TOOLBAR\n environment variable:\n\n\n$ \nENABLE_DEBUG_TOOLBAR\n=\n1\n ./runserver.sh\n\n\n\n\n\nThis tool exposes various useful pieces of information about things like HTTP headers,\nDjango settings, SQL queries, and template variables. Note that running with the toolbar on\nmay have an impact on local server performance.\n\n\nTIP: Updating the documentation\n\n\nOur documentation is written as Markdown files and served in GitHub pages\nby \nmkdocs\n.\n\n\nTo update the docs in GitHub Pages once a pull request has been merged,\nmkdocs provides \na helpful command\n:\n\n\nmkdocs gh-deploy --clean", 
            "title": "Development tips"
        }, 
        {
            "location": "/development-tips/#development-tips", 
            "text": "", 
            "title": "Development tips"
        }, 
        {
            "location": "/development-tips/#tip-updating-npm-shrinkwrapped-dependencies", 
            "text": "For  security reasons ,\nthe dependencies in  package.json  are pinned to a version\nin  npm-shrinkwrap.json .\nThis means updating a project dependency requires updating both files.\nThe easiest way to do this is the following steps:   Update the version of the dependency in  package.json .  Delete the  node_modules  directory.  Delete the  npm-shrinkwrap.json  file.  Run  ./frontend.sh shrinkwrap .   Congrats! The dependency has been updated.   Note  Shrinkwrapping will ignore development dependencies,\nso if one of those was updated you will need to run  npm install \nafter shrinkwrapping to bring your development environment up to date.", 
            "title": "TIP: Updating npm shrinkwrapped dependencies"
        }, 
        {
            "location": "/development-tips/#tip-loading-sibling-projects", 
            "text": "Some projects fit within the cfgov-refresh architecture,\nbut are not fully incorporated into the project.\nThese are known as \"non-v1 Django apps.\"\nIn order to visit areas of the site locally where those projects are used,\nthe sibling projects need to be installed\nand then indexed within this cfgov-refresh project.  The non-v1 apps are the following:   Owning a Home .  fin-ed-resources (ghe/CFGOV/fin-ed-resources) - for the Education Resources section.  know-before-you-owe (ghe/CFGOV/know-before-you-owe) - for the Consumer Tools   Know before you owe section.   After installing these projects as sibling directories to the  cfgov-refresh  repository,", 
            "title": "TIP: Loading sibling projects"
        }, 
        {
            "location": "/development-tips/#option-1-sheer-index-and-elasticsearch-eg-owning-a-home", 
            "text": "build the third-party projects per their directions,  stop the web server and return to  cfgov-refresh  and run  cfgov/manage.py sheer_index -r  to load the projects' data into ElasticSearch.", 
            "title": "Option 1: Sheer Index and Elasticsearch (e.g. owning-a-home)"
        }, 
        {
            "location": "/development-tips/#option-2-direct-dependencies", 
            "text": "Build the third-party projects per their directions  Stop the web server and return to  cfgov-refresh  Run  pip install -e ../ sibling  to load the projects' dependencies    Note  Do not install the projects directly into the  cfgov-refresh  directory.\nClone and install the projects as siblings to  cfgov-refresh ,\nso that they share the same parent directory ( ~/Projects  or similar).", 
            "title": "Option 2: Direct dependencies"
        }, 
        {
            "location": "/development-tips/#tip-loading-data-into-django-models", 
            "text": "The Django management command  import-data  will import data from the specified\nsource into the specified model.  usage: manage.py import-data [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n        [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--parent PARENT]\n        [--snippet] -u USERNAME -p PASSWORD [--app APP] [--overwrite]\n        data_type wagtail_type \n\n-  data_type  is the WordPress post type defined in the  processors.py  file.\n-  wagtail_type  is the Django model name where the data is going to go.\n-  -u  and  -p  are credentials to an admin account.  Required option:   --parent  is the slug of the parent page that the pages will exist\n  under.  --snippet  is a flag that's used to signify that the importing data will be\n  inserted into a Django model, registered as a  Wagtail snippet .\n  One of these options must be set for the command to run.   Other options:   --app  is the name of the app the Django models from  wagtail_type  exist in.\n  It defaults to our app,  v1 .  --overwrite  overwrites existing pages in Wagtail based on comparing slugs.\nBe careful when using this as it will overwrite the data in Wagtail with data\nfrom the source. Default is  False .  --verbosity  is set to 1 by default. Set it to 2 or higher and expect the\nname of the slugs to appear where appropriate.   For now, in order for this command to import the data, one of the things it\nneeds is a file for \"sheer logic\" to use to retrieve the data. For us, the\nprocessors are already done from our last backend. This part of the command\nwill change as we move away from our dependency on \"sheer logic.\" This is set\nby putting the file in a  processors  module in the top level of the project\nand adding it to the setting  SHEER_PROCESSORS .  The command needs a  processors  module in the app that's passed to it, as well\nas a file with the same name as the Django model specified that defines a class\nnamed  DataConverter  that subclasses either  _helpers.PageDataConverter  or _helpers.SnippetDataConverter  and implements their method(s) explained below:  PageDataConverter :   convert(self, imported_data) :\n   For converting pages or snippets, the processor file must implement the\n    convert()  function with one argument. That argument represents the\n   imported data dictionary. That function must take the dictionary and map it\n   to a new one that uses the keys that Wagtail's  create()  and  edit() \n   admin/snippet view functions expect in the  request.POST  dictionary to\n   actually migrate the data over, and then returns that dictionary where it\n   will be assigned to  request.POST .   SnippetDataConverter(PageDataConverter) :   get_existing_snippet() \n   This also accepts the imported data dictionary. It's used to find an\n   existing snippet given the imported data, returning it if found or  None  if\n   not.", 
            "title": "TIP: Loading data into Django models"
        }, 
        {
            "location": "/development-tips/#tip-working-with-the-templates", 
            "text": "", 
            "title": "TIP: Working with the templates"
        }, 
        {
            "location": "/development-tips/#front-end-templateasset-locations", 
            "text": "Templates  that are served by the Django server:  cfgov\\jinja2\\v1  Static assets  prior to processing (minifying etc.):  cfgov\\unprocessed .   Note  After running  gulp build  the site's assets are copied over to  cfgov\\static_built ,\nready to be served by Django.", 
            "title": "Front-End Template/Asset Locations"
        }, 
        {
            "location": "/development-tips/#simple-static-template-setup", 
            "text": "By default, Django will render pages with accordance to the URL pattern defined\nfor it. For example, going to  http://localhost:8000/the-bureau/index.html \n(or  http://localhost:8000/the-bureau/ ) renders  /the-bureau/index.html  from\nthe  cfgov  app folder's  jinja2/v1  templates folder as processed\nby the  Jinja2  templating engine.", 
            "title": "Simple static template setup"
        }, 
        {
            "location": "/development-tips/#tip-outputting-indexed-content-in-a-sheer-template", 
            "text": "Most of our content is indexed from the API output of our WordPress back-end.\nThis happens when the  python cfgov/manage.py sheer_index -r  command is run.  There are two ways in which we use indexed content:\nrepeating items (e.g., blog posts and press releases),\nand single pages (e.g., the Future Requests page in Doing Business with Us).\nWhat follows is a deeper dive into both of these content types.", 
            "title": "TIP: Outputting indexed content in a Sheer template"
        }, 
        {
            "location": "/development-tips/#single-content", 
            "text": "To access a single piece of content,\nthe easiest thing to do is use the  get_document()  function.  Using the example given earlier of the Future Requests page,\nhere's how it's done:  {%   set   page   =   get_document ( pages ,   63169 )   %}  {{   page.content   |   safe   }}   The  get_document  method can be used to retrieve a single item of any post type\nfor display within a template.\nIn the below example we get an instance of the non-hierarchical contact  post type using its slug ( whistleblowers ):  {%   set   whistleblowers   =   get_document ( contact ,   whistleblowers )   %}   In practice, many of our templates are a Frankenstein-type mixture\nof hand-coded static content and calls to indexed content,\nas we continually try to strike the right balance of what content\nis appropriate to be edited by non-developers in Wagtail,\nand what is just too fragile to do any other way than by hand.", 
            "title": "Single content"
        }, 
        {
            "location": "/development-tips/#tip-filtering-results-with-queries", 
            "text": "Sometimes you'll want to create queries in your templates to filter the data.  The two main ways of injecting filters into your data are in the URL's query\nstring and within the template code itself.  We have a handy function  search()  that:   Pulls in filters from the URL query string.  Allows you to add additional filters by passing them in as arguments to the function.", 
            "title": "TIP: Filtering results with queries"
        }, 
        {
            "location": "/development-tips/#url-query-string-filters", 
            "text": "URL query string filters can be further broken down into two types:   Term - Used when you want to filter by whether a field matches a term.\nNote that in order to use this type of filter,\nthe field you are matching it against must have \"index\": \"not_analyzed\"  set in the mapping.  Range - Used for when you want to filter something by a range (e.g. dates or numbers)   An example of Term is:  ?filter_category=Op-Ed  filter_[field]=[value]  An example of Range is:  ?filter_range_date_gte=2014-01  filter_range_[field]_[operator]=[value]  URL query string filters are convenient for many of the filtered queries you'll need to run,\nbut often there are cases where you'll need more flexibility.", 
            "title": "URL query string filters"
        }, 
        {
            "location": "/development-tips/#more-complex-filters", 
            "text": "By default,  search()  uses the default query parameters\ndefined in the  _queries/object-name.json  file,\nthen mixes them in with any additional arguments\nfrom the URL query string in addition to what is passed into the function itself.  When using  search() , you can also pass in filters with the same  filter_  syntax as above.  For example:  search(filter_category='Op-Ed')  Multiple term filters on the same field will be combined in an OR clause, while\nterm filters of different fields will be combined in an AND clause.  For example:  search(filter_tag='Students', filter_tag='Finance', filter_author='Batman')  This will return documents that have the tag Students OR Finance, AND have an author of Batman.  If you need more control over your filter than that,\nenter it manually in the  cfgov/jinja2/v1/_queries/[filtername].json  file.", 
            "title": "More complex filters"
        }, 
        {
            "location": "/development-tips/#tip-debugging-site-performance", 
            "text": "When running locally it is possible to enable the Django Debug Toolbar \nby defining the  ENABLE_DEBUG_TOOLBAR  environment variable:  $  ENABLE_DEBUG_TOOLBAR = 1  ./runserver.sh  This tool exposes various useful pieces of information about things like HTTP headers,\nDjango settings, SQL queries, and template variables. Note that running with the toolbar on\nmay have an impact on local server performance.", 
            "title": "TIP: Debugging site performance"
        }, 
        {
            "location": "/development-tips/#tip-updating-the-documentation", 
            "text": "Our documentation is written as Markdown files and served in GitHub pages\nby  mkdocs .  To update the docs in GitHub Pages once a pull request has been merged,\nmkdocs provides  a helpful command :  mkdocs gh-deploy --clean", 
            "title": "TIP: Updating the documentation"
        }
    ]
}